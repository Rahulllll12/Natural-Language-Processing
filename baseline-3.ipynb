{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11422095,"sourceType":"datasetVersion","datasetId":7153367}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow\n!pip install -U imbalanced-learn\n!pip uninstall scikit-learn imbalanced-learn sklearn-compat -y\npip install scikit-learn==1.4.2 imbalanced-learn==0.12.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers datasets accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:48:11.117988Z","iopub.execute_input":"2025-04-15T15:48:11.118329Z","iopub.status.idle":"2025-04-15T15:49:24.387477Z","shell.execute_reply.started":"2025-04-15T15:48:11.118303Z","shell.execute_reply":"2025-04-15T15:49:24.386428Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting accelerate\n  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, accelerate\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pip install scikit-learn==1.4.2 imbalanced-learn==0.12.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nDiplomacy Message Classification - Training Script V3.1 (GRU Removed)\nModels: TF-IDF+SMOTE+(ML Models+Ensemble) AND LSTM with GloVe.\nBased on SENDER LABELS. Focus on minority F1 score.\n\n**MODIFIED: Removed GRU model.**\n**MODIFIED: Added SMOTE, Threshold Tuning, Custom Ensemble.**\n**MODIFIED: Fine-tuning embeddings (trainable=True).**\n\"\"\"\n\nimport json\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport time\nimport joblib\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport sys # For checking executable path\n\n# ML Imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.tree import DecisionTreeClassifier # Removed for brevity\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier # Removed GB for brevity\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, precision_recall_curve\nfrom sklearn.utils import resample, class_weight as sk_class_weight\n\n# Imbalance Handling\nfrom imblearn.over_sampling import SMOTE\n\n# DL Imports\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer # Alias to avoid confusion\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\n# Removed GRU from layers import\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping as KerasEarlyStopping # Alias\n# import matplotlib.pyplot as plt # Optional for plotting\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\n# Make sure imblearn is installed: pip install -U imbalanced-learn\n\n# --- Configuration ---\n# File Paths\nTRAIN_FILE_PATH = \"/kaggle/input/nlpdata/train.jsonl\" #<--- CHECK/REPLACE\nVALIDATION_FILE_PATH = \"/kaggle/input/nlpdata/validation.jsonl\" #<--- CHECK/REPLACE\nTEST_FILE_PATH = \"/kaggle/input/nlpdata/test.jsonl\" #<--- CHECK/REPLACE\nGLOVE_EMBEDDING_PATH = \"/kaggle/input/nlpdata/glove.6B.100d.txt\" # <--- !!!!!!!!!!! CHECK/REPLACE THIS PATH !!!!!!!!!!!\nSAVE_DIR = \"trained_models_v3.1_smote_lstm_ensemble\" # Updated save dir name\n\n# General Parameters\nRANDOM_STATE = 42\nVALIDATION_SET_SIZE = 0.15 # Hold out 15% of balanced data for validation/threshold tuning\nNUM_TEST_EXAMPLES_TO_SHOW = 5\n\n# TF-IDF / ML Parameters\nTFIDF_MAX_FEATURES = 5000\nNGRAM_RANGE = (1, 2)\n\n# Deep Learning Parameters\nVOCAB_SIZE = 10000\nMAX_LENGTH = 100\nEMBEDDING_DIM = 100\nLSTM_UNITS = 64\n# GRU_UNITS = 64 # <--- REMOVED\nDROPOUT_RATE = 0.4\nSPATIAL_DROPOUT_RATE = 0.4\nDL_EPOCHS = 6\nDL_BATCH_SIZE = 64\nDL_EARLY_STOPPING_PATIENCE = 3\n\n# --- NLTK Data Check ---\ntry:\n    stopwords.words('english')\nexcept LookupError:\n    print(\"NLTK stopwords download.\")\n    nltk.download('stopwords', quiet=True)\ntry:\n    word_tokenize(\"test sentence\")\nexcept LookupError:\n    print(\"NLTK punkt download.\")\n    nltk.download('punkt', quiet=True)\n\n# --- Helper Functions ---\ndef load_jsonl(file_path):\n    \"\"\"Loads data from a JSON Lines file.\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        # Attempt common Kaggle path structure\n        base_name = os.path.basename(file_path)\n        dir_name = os.path.basename(os.path.dirname(file_path))\n        kaggle_path_guess1 = f\"/kaggle/input/{base_name}\"\n        kaggle_path_guess2 = f\"/kaggle/input/{dir_name}/{base_name}\"\n        print(f\"File not found at '{file_path}'. Trying Kaggle paths...\")\n        if os.path.exists(kaggle_path_guess1):\n            print(f\"Found at: '{kaggle_path_guess1}'\")\n            file_path = kaggle_path_guess1\n        elif os.path.exists(kaggle_path_guess2):\n            print(f\"Found at: '{kaggle_path_guess2}'\")\n            file_path = kaggle_path_guess2\n        else:\n            raise FileNotFoundError(f\"❌ Data file not found at '{file_path}' or likely Kaggle paths ('{kaggle_path_guess1}', '{kaggle_path_guess2}').\")\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line_num, line in enumerate(f):\n                try:\n                    data.append(json.loads(line))\n                except json.JSONDecodeError as e:\n                    print(f\"Warn: Skip JSON line {line_num+1}: {e}\\nLine: {line.strip()}\")\n        print(f\"✅ Loaded {len(data)} entries from {file_path}.\")\n        return data\n    except Exception as e:\n        raise RuntimeError(f\"❌ Error loading {file_path}: {e}\")\n\ndef data_to_dataframe(jsonl_data):\n    \"\"\"Converts loaded JSONL data into a message DataFrame using SENDER_LABELS.\"\"\"\n    all_messages = []\n    all_sender_labels = []\n    game_ids = []\n    relative_indices = []\n    print(\"Processing data using 'sender_labels' as ground truth...\")\n    for i, game_data in enumerate(jsonl_data):\n        game_id = game_data.get(\"game_id\", f\"UNKNOWN_{i}\")\n        if not isinstance(game_data, dict) or \"messages\" not in game_data or \"sender_labels\" not in game_data:\n            print(f\"Warn: Skip game (idx {i}): Lacks 'messages' or 'sender_labels'. Keys: {list(game_data.keys()) if isinstance(game_data, dict) else 'Not a dict'}\")\n            continue\n        messages = game_data[\"messages\"]\n        sender_labels = game_data[\"sender_labels\"]\n        if not isinstance(messages, list) or not isinstance(sender_labels, list):\n            print(f\"Warn: 'messages'/'sender_labels' not lists in game {game_id}. Skip.\")\n            continue\n        num_messages = len(messages)\n        num_labels = len(sender_labels)\n        if num_messages != num_labels:\n            print(f\"Warn: Mismatch msg({num_messages})/sender_label({num_labels}) count for game {game_id}. Skip.\")\n            continue\n        # Use provided relative index or generate default\n        rel_idx = game_data.get(\"relative_message_index\", list(range(num_messages)))\n        if not isinstance(rel_idx, list) or len(rel_idx) != num_messages:\n            print(f\"Warn: Mismatch/Invalid relative index for game {game_id}. Using default range.\")\n            rel_idx = list(range(num_messages))\n\n        all_messages.extend(messages)\n        all_sender_labels.extend(sender_labels)\n        game_ids.extend([game_id] * num_messages)\n        relative_indices.extend(rel_idx)\n\n    if not all_messages:\n        print(\"Warn: No valid messages found in data.\")\n        return pd.DataFrame()\n\n    df = pd.DataFrame({\"game_id\": game_ids, \"relative_index\": relative_indices, \"messages\": all_messages, \"labels\": all_sender_labels})\n    # Ensure 'labels' are consistently 0 or 1\n    df['labels'] = df['labels'].apply(lambda x: 1 if str(x).lower() == 'true' else 0)\n    print(f\"DataFrame created with {len(df)} messages (using sender_labels).\")\n    if not df.empty:\n        print(f\"Label distribution (sender):\\n{df['labels'].value_counts(normalize=True)}\")\n    return df\n\ndef preprocess_text(text):\n    \"\"\"Basic text preprocessing: lowercase, remove URLs, HTML tags, non-alpha chars, extra whitespace.\"\"\"\n    if not isinstance(text, str):\n        text = str(text) # Ensure input is string\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # Remove non-alphabetic characters (keeping spaces)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef load_glove_embeddings(embedding_path):\n    \"\"\"Loads GloVe embeddings from a file into a dictionary.\"\"\"\n    print(f\"Loading GloVe embeddings from: {embedding_path}\")\n    embeddings_index = {}\n    try:\n        with open(embedding_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                try:\n                    coefs = np.asarray(values[1:], dtype='float32')\n                    if len(coefs) == EMBEDDING_DIM: # Ensure dimension match\n                         embeddings_index[word] = coefs\n                    else:\n                         # print(f\"Warn: Skipping word '{word}' - embedding dim mismatch ({len(coefs)} vs {EMBEDDING_DIM})\") # Optional warning\n                         pass\n                except ValueError:\n                     # print(f\"Warn: Skipping line, cannot parse vectors: {line[:50]}...\") # Optional warning\n                     pass # Ignore lines that are not valid word vectors\n        print(f\"✅ Found {len(embeddings_index)} word vectors with dim {EMBEDDING_DIM}.\")\n        if not embeddings_index: print(\"Warning: No embeddings loaded. Check path and dimensions.\")\n        return embeddings_index\n    except FileNotFoundError:\n        print(f\"❌ Error: GloVe file not found: {embedding_path}. Update GLOVE_EMBEDDING_PATH.\")\n        return None\n    except Exception as e:\n        print(f\"❌ Error loading GloVe embeddings: {e}\")\n        return None\n\n# --- DL Model Definition Functions ---\ndef build_lstm_model(vocab_size, embedding_dim, max_length, embedding_matrix=None):\n    \"\"\"Builds a Bidirectional LSTM model with trainable embeddings option.\"\"\"\n    print(\"Building LSTM Model...\")\n    model = Sequential(name=\"LSTM_Model\")\n    if embedding_matrix is not None:\n        print(\"Using pre-trained GloVe embeddings (fine-tuning enabled).\")\n        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n    else:\n        print(\"Training embeddings from scratch.\")\n        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True))\n    model.add(SpatialDropout1D(SPATIAL_DROPOUT_RATE))\n    model.add(Bidirectional(LSTM(LSTM_UNITS, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\n    model.add(Dense(1, activation='sigmoid'))\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    # Include Recall/Precision for Truthful class directly in compile for monitoring during fit\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy',\n                                                                            tf.keras.metrics.Recall(class_id=0, name='recall_truthful'),\n                                                                            tf.keras.metrics.Precision(class_id=0, name='precision_truthful')])\n    print(model.summary())\n    return model\n\n# --- build_gru_model function REMOVED ---\n\n# --- Evaluation Helper ---\ndef print_evaluation_metrics(y_true, y_pred, model_name, target_names, threshold=0.5):\n    \"\"\"Calculates and prints detailed classification metrics using a specific threshold.\"\"\"\n    print(f\"\\n--- Evaluation: {model_name} (Threshold: {threshold:.3f}) ---\") # Increased precision for threshold\n    # Ensure y_true and y_pred are numpy arrays for consistent processing\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    if len(y_true) == 0 or len(y_pred) == 0:\n         print(\"Error: Empty true labels or predictions array.\")\n         return {\"accuracy\":0.0,\"report\":\"Error\",\"confusion_matrix\":None}\n    if len(y_true) != len(y_pred):\n         print(f\"Error: Label ({len(y_true)}) and prediction ({len(y_pred)}) length mismatch.\")\n         return {\"accuracy\":0.0,\"report\":\"Error\",\"confusion_matrix\":None}\n\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(\"\\nClassification Report:\")\n    try:\n        # Check unique labels present in BOTH y_true and y_pred for report generation\n        unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n        report_labels = [0, 1] # Assume standard binary case for report\n\n        if len(unique_labels) < 2:\n             print(f\"Warn: Only one class ({unique_labels}) present in true/pred. Report may be partial.\")\n             # Adjust target names if only one class is predicted/present\n             report_labels = unique_labels.tolist()\n             current_target_names = [target_names[int(l)] for l in report_labels if l < len(target_names)]\n             if not current_target_names: current_target_names = [f\"Class {l}\" for l in report_labels]\n        else:\n             current_target_names = target_names # Use standard names for binary case\n\n        report_text = classification_report(y_true, y_pred, labels=report_labels, target_names=current_target_names, zero_division=0, output_dict=False)\n        report_dict = classification_report(y_true, y_pred, labels=report_labels, target_names=current_target_names, zero_division=0, output_dict=True)\n        print(report_text)\n\n        # Safely extract F1 scores, defaulting to 0.0 if class 0 or 1 not in report_dict\n        truth_f1 = report_dict.get(target_names[0], {}).get('f1-score', 0.0)\n        lie_f1 = report_dict.get(target_names[1], {}).get('f1-score', 0.0)\n        macro_f1 = report_dict.get('macro avg', {}).get('f1-score', 0.0)\n        weighted_f1 = report_dict.get('weighted avg', {}).get('f1-score', 0.0)\n\n        print(\"\\nKey F1-Scores:\")\n        print(f\"  F1 Score [{target_names[0]}]: {truth_f1:.4f}\")\n        print(f\"  F1 Score [{target_names[1]}]: {lie_f1:.4f}\")\n        print(f\"  F1 Score [Macro Avg]:     {macro_f1:.4f}\")\n        print(f\"  F1 Score [Weighted Avg]:  {weighted_f1:.4f}\")\n\n        print(\"\\nConfusion Matrix:\")\n        cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) # Force labels for standard TN/FP layout\n        tn, fp, fn, tp = cm.ravel()\n        print(f\"            Predicted:\")\n        print(f\"            {target_names[0]:<12} {target_names[1]:<12}\")\n        print(f\"Actual {target_names[0]:<10} [{tn:<12} {fp:<12}] (TN, FP)\")\n        print(f\"Actual {target_names[1]:<10} [{fn:<12} {tp:<12}] (FN, TP)\")\n        return {\"accuracy\": accuracy, \"report\": report_dict, \"confusion_matrix\": cm, \"f1_truthful\": truth_f1, \"f1_deceptive\": lie_f1, \"f1_macro\": macro_f1}\n\n    except Exception as e:\n        print(f\"❌ Error generating classification report/CM for {model_name}: {e}\")\n        try: acc = accuracy_score(y_true, y_pred)\n        except: acc = 0.0\n        return {\"accuracy\":acc,\"report\":\"Error\",\"confusion_matrix\":None, \"f1_truthful\": 0.0, \"f1_deceptive\": 0.0, \"f1_macro\": 0.0}\n\ndef find_best_threshold(y_true, y_proba, target_class_index=0, metric='f1'):\n    \"\"\"Finds the best threshold to maximize F1 score for the target class (default: class 0 - Truthful).\"\"\"\n    best_threshold = 0.5\n    best_score = -1\n\n    y_true = np.asarray(y_true)\n    y_proba = np.asarray(y_proba)\n\n    if len(y_true) != len(y_proba):\n        print(f\"Warn: Label ({len(y_true)}) and probability ({len(y_proba)}) length mismatch in find_best_threshold. Using default 0.5.\")\n        return 0.5, -1\n    if not np.all(np.isfinite(y_proba)):\n        print(\"Warn: Non-finite values (NaN/inf) found in probabilities. Using default 0.5 threshold.\")\n        return 0.5, -1\n    if len(np.unique(y_true)) < 2:\n         print(f\"Warn: Only one class found in y_true for threshold tuning. Using default 0.5 threshold.\")\n         return 0.5, -1 # Cannot calculate PR curve with only one class\n\n    try:\n        precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n    except ValueError as prc_e:\n        print(f\"Warn: Could not calculate precision-recall curve: {prc_e}. Using default 0.5 threshold.\")\n        return 0.5, -1\n\n    print(f\"Finding best threshold for class {target_class_index} ({target_names[target_class_index]}) using F1 score...\")\n    best_score = -1\n    best_threshold = 0.5 # Default\n\n    for threshold_candidate in np.linspace(0.01, 0.99, 99):\n        y_pred_tuned = (y_proba >= threshold_candidate).astype(int)\n        current_f1 = f1_score(y_true, y_pred_tuned, pos_label=target_class_index, average='binary', zero_division=0)\n        if current_f1 > best_score:\n            best_score = current_f1\n            best_threshold = threshold_candidate\n\n    # Check the default 0.5 threshold as well\n    y_pred_05 = (y_proba >= 0.5).astype(int)\n    f1_05 = f1_score(y_true, y_pred_05, pos_label=target_class_index, average='binary', zero_division=0)\n    if f1_05 > best_score:\n         best_score = f1_05\n         best_threshold = 0.5\n\n    print(f\"Best threshold found: {best_threshold:.3f} (Maximizes F1 for class {target_class_index} at {best_score:.4f})\")\n    return best_threshold, best_score\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n\n    # Ensure target names are defined early and consistently\n    target_names = [\"Truthful (0)\", \"Deceptive (1)\"]\n\n    # --- 1. Load Data ---\n    print(\"--- Loading Data ---\")\n    try:\n        train_data = load_jsonl(TRAIN_FILE_PATH)\n        val_data = load_jsonl(VALIDATION_FILE_PATH)\n        train_val_data = train_data + val_data\n        test_data = load_jsonl(TEST_FILE_PATH)\n        del train_data, val_data\n        gc.collect()\n    except (FileNotFoundError, RuntimeError) as e:\n        print(e)\n        exit(1) # Exit if essential data is missing\n    if not train_val_data:\n        print(\"❌ Error: No train/validation data loaded. Exiting.\")\n        exit(1)\n    if not test_data:\n        print(\"Warning: No test data loaded. Evaluation on test set will be skipped.\")\n\n    # --- 2. Prepare DataFrames ---\n    print(\"\\n--- Preparing DataFrames (using sender_labels) ---\")\n    train_val_df = data_to_dataframe(train_val_data)\n    test_df = data_to_dataframe(test_data) if test_data else pd.DataFrame()\n    del train_val_data, test_data # Free memory\n    gc.collect()\n    if train_val_df.empty:\n        print(\"❌ Error: Train+Validation DataFrame is empty after processing. Exiting.\")\n        exit(1)\n\n    # --- 3. Initial Balancing & Train/Validation Split ---\n    print(\"\\n--- Initial Balancing & Train/Validation Split ---\")\n    label_counts = train_val_df['labels'].value_counts()\n    train_df_balanced = pd.DataFrame() # Initialize\n\n    if len(label_counts) < 2:\n        print(\"Warning: Only one class found in the combined train/val data. Skipping initial balancing.\")\n        train_df_balanced = train_val_df.copy()\n    else:\n        majority_class_label = label_counts.idxmax()\n        minority_class_label = label_counts.idxmin()\n        majority_count = label_counts.max()\n        minority_count = label_counts.min()\n\n        if minority_count <= 0:\n             print(f\"Error: Minority class ({minority_class_label}) has zero samples. Cannot balance.\")\n             train_df_balanced = train_val_df.copy()\n        elif majority_count == minority_count:\n            print(\"Data is already balanced. Skipping initial upsampling.\")\n            train_df_balanced = train_val_df.copy()\n        else:\n            print(f\"Initial Upsampling minority class ({minority_class_label}) from {minority_count} to {majority_count}.\")\n            df_minority = train_val_df[train_val_df.labels == minority_class_label]\n            df_majority = train_val_df[train_val_df.labels == majority_class_label]\n            df_minority_upsampled = resample(df_minority, replace=True, n_samples=majority_count, random_state=RANDOM_STATE)\n            train_df_balanced = pd.concat([df_majority, df_minority_upsampled])\n            del df_minority, df_majority, df_minority_upsampled\n            gc.collect()\n\n    train_df_balanced = train_df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n    print(f\"Initially balanced dataset size: {len(train_df_balanced)}\")\n    del train_val_df\n    gc.collect()\n\n    df_train, df_val = train_test_split(\n        train_df_balanced,\n        test_size=VALIDATION_SET_SIZE,\n        random_state=RANDOM_STATE,\n        stratify=train_df_balanced['labels']\n    )\n    print(f\"Final Train Set Size: {len(df_train)}, Validation Set Size: {len(df_val)}\")\n    print(f\"Train label distribution:\\n{df_train['labels'].value_counts(normalize=True)}\")\n    print(f\"Validation label distribution:\\n{df_val['labels'].value_counts(normalize=True)}\")\n    del train_df_balanced\n    gc.collect()\n\n    train_texts_raw = df_train['messages'].tolist()\n    y_train = np.array(df_train['labels'].tolist())\n    val_texts_raw = df_val['messages'].tolist()\n    y_val = np.array(df_val['labels'].tolist())\n    test_texts_raw = []\n    y_test = np.array([])\n    if not test_df.empty:\n        test_texts_raw = test_df['messages'].tolist()\n        y_test = np.array(test_df['labels'].tolist())\n    else:\n        print(\"Warning: Test DataFrame is empty or was not loaded.\")\n    print(f\"Data shapes: Train Text={len(train_texts_raw)}, Train Labels={y_train.shape}\")\n    print(f\"             Val Text={len(val_texts_raw)}, Val Labels={y_val.shape}\")\n    print(f\"             Test Text={len(test_texts_raw)}, Test Labels={y_test.shape}\")\n\n    # --- 4. Text Preprocessing ---\n    print(\"\\n--- Text Preprocessing ---\")\n    start_preprocess = time.time()\n    train_texts_processed = [preprocess_text(text) for text in train_texts_raw]\n    val_texts_processed = [preprocess_text(text) for text in val_texts_raw]\n    test_texts_processed = [preprocess_text(text) for text in test_texts_raw] if test_texts_raw else []\n    print(f\"Text preprocessing took {time.time() - start_preprocess:.2f} seconds.\")\n    del train_texts_raw, val_texts_raw, test_texts_raw\n    gc.collect()\n\n    # --- Initialize Result Storage ---\n    ml_models = {}\n    ml_results = {}\n    ml_probas_val = {}\n    ml_probas_test = {}\n    vectorizer = None\n    dl_models = {} # Will store LSTM model\n    dl_results = {}\n    dl_probas_val = {}\n    dl_probas_test = {}\n    keras_tokenizer = None\n    embedding_matrix = None\n    best_thresholds = {}\n    custom_ensemble_results = {}\n    avg_proba_test = None\n    avg_threshold = 0.5\n\n    # =====================================================\n    # == Section A: TF-IDF + ML Models Pipeline          ==\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section A: TF-IDF + ML Models Pipeline\\n\" + \"=\"*50)\n    X_train_ml, y_train_ml = None, None\n    X_val_ml, y_val_ml = None, None\n    X_test_ml = None\n\n    try:\n        # --- A.1 TF-IDF ---\n        print(\"\\n--- A.1 Feature Extraction (TF-IDF) ---\")\n        start_time = time.time()\n        vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=NGRAM_RANGE, stop_words='english', min_df=5)\n        print(\"Fitting TF-IDF on training data...\")\n        X_train_tfidf = vectorizer.fit_transform(train_texts_processed)\n        print(\"Transforming validation and test data...\")\n        X_val_ml = vectorizer.transform(val_texts_processed)\n        y_val_ml = y_val\n        X_test_ml = vectorizer.transform(test_texts_processed) if test_texts_processed else None\n        print(f\"TF-IDF transformation took {time.time() - start_time:.2f}s.\")\n        print(f\"TF-IDF Shapes: Train={X_train_tfidf.shape}, Val={X_val_ml.shape}\", end=\"\")\n        if X_test_ml is not None: print(f\", Test={X_test_ml.shape}\")\n        else: print(\", No test data.\")\n\n        # --- A.2 Apply SMOTE ---\n        print(\"\\n--- A.2 Applying SMOTE to TF-IDF Training Data ---\")\n        start_time = time.time()\n        unique_train_labels, counts_train_labels = np.unique(y_train, return_counts=True)\n        min_class_count_train = counts_train_labels.min() if len(counts_train_labels)>0 else 0\n        if len(unique_train_labels) < 2 or min_class_count_train < 6:\n             print(f\"Warn: Skipping SMOTE (only one class or minority count < 6).\")\n             X_train_ml, y_train_ml = X_train_tfidf, y_train\n        else:\n             print(\"Applying SMOTE...\")\n             smote = SMOTE(random_state=RANDOM_STATE, n_jobs=-1)\n             try:\n                 X_train_ml, y_train_ml = smote.fit_resample(X_train_tfidf, y_train)\n                 print(f\"SMOTE took {time.time() - start_time:.2f}s. Shape: {X_train_ml.shape}\")\n                 print(f\"Label distribution after SMOTE:\\n{pd.Series(y_train_ml).value_counts(normalize=True)}\")\n             except Exception as smote_err:\n                 print(f\"❌ Error during SMOTE: {smote_err}. Using original training data for ML.\")\n                 X_train_ml, y_train_ml = X_train_tfidf, y_train\n        del X_train_tfidf\n        gc.collect()\n\n        # --- A.3 Train/Evaluate ML Models ---\n        print(\"\\n--- A.3 Training and Evaluating ML Models ---\")\n        classifiers = {\n            \"Logistic Regression\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, solver='liblinear'),\n            \"Random Forest\": RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100, n_jobs=-1),\n            \"Multinomial NB\": MultinomialNB(),\n        }\n        for name, model in classifiers.items():\n            print(f\"\\nTraining {name}...\")\n            start_time = time.time()\n            try:\n                model.fit(X_train_ml, y_train_ml)\n                print(f\"✅ Training completed in {time.time() - start_time:.2f} seconds.\")\n                ml_models[name] = model\n                if X_test_ml is not None and len(y_test) > 0:\n                    preds_test = model.predict(X_test_ml)\n                    results = print_evaluation_metrics(y_test, preds_test, f\"{name} @0.5 (Test)\", target_names, threshold=0.5)\n                    ml_results[name] = results\n                else:\n                    print(f\"Skipping TEST set evaluation for {name}.\")\n                    ml_results[name] = None\n                if hasattr(model, \"predict_proba\"):\n                    try:\n                        ml_probas_val[name] = model.predict_proba(X_val_ml)[:, 1]\n                        if X_test_ml is not None: ml_probas_test[name] = model.predict_proba(X_test_ml)[:, 1]\n                        else: ml_probas_test[name] = None\n                    except Exception as pp_e:\n                        print(f\"Warn: Could not get predict_proba for {name}: {pp_e}\")\n                        ml_probas_val[name], ml_probas_test[name] = None, None\n                else:\n                    print(f\"Warn: {name} does not support predict_proba.\")\n                    ml_probas_val[name], ml_probas_test[name] = None, None\n            except Exception as e:\n                print(f\"❌ Error training/evaluating {name}: {e}\")\n                ml_models[name], ml_results[name], ml_probas_val[name], ml_probas_test[name] = None, None, None, None\n                gc.collect()\n\n        # --- A.4 Voting Ensemble ---\n        print(\"\\n--- A.4 Setting up and Evaluating ML Voting Ensemble ---\")\n        estimators = []\n        for name, model in ml_models.items():\n            if model is not None and ml_probas_val.get(name) is not None: # Check probas exist for soft voting\n                estimators.append((name, model))\n            else: print(f\"Warn: Excluding '{name}' from soft voting ensemble.\")\n        if len(estimators) >= 2:\n            print(f\"Creating Soft Voting Ensemble with: {', '.join([n for n,_ in estimators])}\")\n            ensemble_soft = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n            print(\"Fitting Soft Voting Ensemble...\")\n            start_time = time.time()\n            try:\n                ensemble_soft.fit(X_train_ml, y_train_ml)\n                print(f\"✅ Ensemble fitting completed in {time.time() - start_time:.2f} seconds.\")\n                ml_models[\"Ensemble (Soft)\"] = ensemble_soft\n                if X_test_ml is not None and len(y_test) > 0:\n                    ensemble_preds_test = ensemble_soft.predict(X_test_ml)\n                    results = print_evaluation_metrics(y_test, ensemble_preds_test, \"Ensemble (Soft) @0.5 (Test)\", target_names, threshold=0.5)\n                    ml_results[\"Ensemble (Soft)\"] = results\n                else:\n                    print(\"Skipping TEST evaluation for ML Ensemble.\")\n                    ml_results[\"Ensemble (Soft)\"] = None\n                try:\n                    ml_probas_val[\"Ensemble (Soft)\"] = ensemble_soft.predict_proba(X_val_ml)[:, 1]\n                    if X_test_ml is not None: ml_probas_test[\"Ensemble (Soft)\"] = ensemble_soft.predict_proba(X_test_ml)[:, 1]\n                    else: ml_probas_test[\"Ensemble (Soft)\"] = None\n                except Exception as pp_e:\n                    print(f\"Warn: Could not get predict_proba for ML Ensemble: {pp_e}\")\n                    ml_probas_val[\"Ensemble (Soft)\"], ml_probas_test[\"Ensemble (Soft)\"] = None, None\n            except Exception as e:\n                print(f\"❌ Error fitting/evaluating ML Ensemble: {e}\")\n                ml_models[\"Ensemble (Soft)\"], ml_results[\"Ensemble (Soft)\"], ml_probas_val[\"Ensemble (Soft)\"], ml_probas_test[\"Ensemble (Soft)\"] = None, None, None, None\n                gc.collect()\n        else:\n            print(\"Skipping ML Voting Ensemble (need >= 2 valid base models).\")\n            ml_models[\"Ensemble (Soft)\"], ml_results[\"Ensemble (Soft)\"], ml_probas_val[\"Ensemble (Soft)\"], ml_probas_test[\"Ensemble (Soft)\"] = None, None, None, None\n\n    except Exception as pipeline_e:\n        print(f\"\\n❌❌❌ ERROR in TF-IDF/ML Pipeline: {pipeline_e} ❌❌❌\")\n    finally:\n        del X_train_ml, y_train_ml, X_val_ml, y_val_ml, X_test_ml\n        gc.collect()\n\n    # =====================================================\n    # == Section B: Deep Learning Model Pipeline (LSTM Only) == # <--- Modified Section Title\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section B: Deep Learning Model Pipeline (LSTM Only)\\n\" + \"=\"*50) # <--- Modified Section Title\n    X_train_dl, X_val_dl, X_test_dl = None, None, None # Initialize\n\n    try:\n        # --- B.1 Tokenization & Padding ---\n        print(\"\\n--- B.1 Tokenization & Padding for LSTM ---\") # <--- Modified Title\n        start_time = time.time()\n        keras_tokenizer = KerasTokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\") # Renamed variable\n        keras_tokenizer.fit_on_texts(train_texts_processed)\n        word_index = keras_tokenizer.word_index\n        print(f\"Found {len(word_index)} unique tokens in Keras tokenizer.\")\n        train_sequences = keras_tokenizer.texts_to_sequences(train_texts_processed)\n        val_sequences = keras_tokenizer.texts_to_sequences(val_texts_processed)\n        test_sequences = keras_tokenizer.texts_to_sequences(test_texts_processed) if test_texts_processed else []\n        X_train_dl = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n        X_val_dl = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n        X_test_dl = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post') if test_sequences else None\n        print(f\"Keras Tokenization/Padding took {time.time() - start_time:.2f}s.\")\n        print(f\"Padded Shapes: Train={X_train_dl.shape}, Val={X_val_dl.shape}\", end=\"\")\n        if X_test_dl is not None: print(f\", Test={X_test_dl.shape}\")\n        else: print(\", No test data.\")\n        del train_sequences, val_sequences, test_sequences\n        gc.collect()\n\n        # --- B.2 Load Embeddings & Create Matrix ---\n        print(\"\\n--- B.2 Preparing GloVe Embedding Matrix for LSTM ---\") # <--- Modified Title\n        start_time = time.time()\n        embeddings_index = load_glove_embeddings(GLOVE_EMBEDDING_PATH)\n        if embeddings_index:\n            embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n            hits, misses = 0, 0\n            for word, i in word_index.items():\n                if i >= VOCAB_SIZE: continue\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i], hits = embedding_vector, hits + 1\n                else: misses += 1\n            print(f\"Embedding matrix created in {time.time() - start_time:.2f}s. {hits} hits, {misses} misses.\")\n            del embeddings_index; gc.collect()\n        else:\n             print(\"GloVe embeddings failed. LSTM trains embeddings from scratch.\")\n             embedding_matrix = None\n\n        # --- B.3 Define Class Weights & Early Stopping ---\n        print(\"\\n--- B.3 Calculating Class Weights & Setting Early Stopping ---\")\n        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n        dl_class_weight_dict = None\n        if len(unique_classes) == 2:\n             dl_class_weights_computed = sk_class_weight.compute_class_weight('balanced', classes=unique_classes, y=y_train)\n             dl_class_weight_dict = dict(zip(unique_classes, dl_class_weights_computed))\n             print(f\"Calculated DL Class Weights: {dl_class_weight_dict}\")\n        else: print(\"Warn: Not using class weights (y_train not binary).\")\n        dl_early_stopping = KerasEarlyStopping(monitor='val_loss', patience=DL_EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1)\n\n        # --- B.4 Train & Evaluate LSTM --- # <--- Renumbered Section\n        print(\"\\n--- B.4 Training LSTM Model ---\") # <--- Renumbered Section\n        lstm_model = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH, embedding_matrix)\n        start_time = time.time()\n        history_lstm = lstm_model.fit(X_train_dl, y_train, epochs=DL_EPOCHS, batch_size=DL_BATCH_SIZE,\n                                      validation_data=(X_val_dl, y_val),\n                                      callbacks=[dl_early_stopping], class_weight=dl_class_weight_dict, verbose=1)\n        train_time = time.time() - start_time\n        stopped_epoch_lstm = dl_early_stopping.stopped_epoch\n        print(f\"✅ LSTM Training completed in {train_time:.2f} seconds (stopped epoch: {stopped_epoch_lstm if stopped_epoch_lstm > 0 else 'Not stopped early'}).\")\n        dl_models[\"LSTM\"] = lstm_model # Store trained model\n\n        # Evaluate LSTM\n        if X_test_dl is not None and len(y_test) > 0:\n            print(\"\\nEvaluating LSTM on Test Set...\")\n            start_time = time.time()\n            dl_probas_test[\"LSTM\"] = lstm_model.predict(X_test_dl, verbose=0).flatten()\n            test_preds_lstm_05 = (dl_probas_test[\"LSTM\"] > 0.5).astype(int)\n            print(f\"LSTM Prediction took {time.time() - start_time:.2f}s.\")\n            results = print_evaluation_metrics(y_test, test_preds_lstm_05, \"LSTM @0.5 (Test)\", target_names, threshold=0.5)\n            dl_results[\"LSTM\"] = results\n            print(\"Getting LSTM probabilities on Validation Set...\")\n            dl_probas_val[\"LSTM\"] = lstm_model.predict(X_val_dl, verbose=0).flatten()\n        else:\n            print(\"Skipping LSTM evaluation on test set.\")\n            dl_results[\"LSTM\"], dl_probas_val[\"LSTM\"], dl_probas_test[\"LSTM\"] = None, None, None\n\n        # --- GRU Section Removed ---\n\n    except Exception as pipeline_e:\n        print(f\"\\n❌❌❌ ERROR in DL (LSTM) Pipeline: {pipeline_e} ❌❌❌\")\n        if \"LSTM\" not in dl_models: dl_models[\"LSTM\"] = None # Ensure marked as None if error before storage\n    finally:\n         del X_train_dl, X_val_dl, X_test_dl\n         if 'embedding_matrix' in locals(): del embedding_matrix\n         gc.collect()\n\n    # =====================================================\n    # == Section C: Threshold Tuning (using Validation Set) ==\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section C: Threshold Tuning (using Validation Set)\\n\" + \"=\"*50)\n    # Combine probabilities from ALL models that produced validation probabilities\n    # GRU key will simply not exist in dl_probas_val if it wasn't run\n    all_val_probas = {**ml_probas_val, **dl_probas_val}\n\n    if not y_val.size:\n        print(\"Validation set (y_val) is empty. Skipping threshold tuning.\")\n    elif not all_val_probas:\n        print(\"No model probabilities available for validation set. Skipping threshold tuning.\")\n    else:\n        print(f\"Finding best thresholds for models: {list(all_val_probas.keys())}\")\n        for name, y_proba_val in all_val_probas.items():\n            if y_proba_val is None or not isinstance(y_proba_val, np.ndarray) or not len(y_proba_val):\n                print(f\"Skipping threshold tuning for '{name}' (no valid probabilities).\")\n                best_thresholds[name] = 0.5\n                continue\n            if len(y_proba_val) != len(y_val):\n                print(f\"Warn: Skipping threshold tuning for '{name}'. Proba length mismatch.\")\n                best_thresholds[name] = 0.5\n                continue\n\n            print(f\"\\nTuning threshold for {name}...\")\n            best_thresh, best_f1 = find_best_threshold(y_val, y_proba_val, target_class_index=0, metric='f1')\n            best_thresholds[name] = best_thresh\n\n    print(\"\\nBest Thresholds found (optimizing F1 for Truthful class on Validation Set):\")\n    for name, thresh in best_thresholds.items(): print(f\"  {name:<25}: {thresh:.3f}\")\n\n\n    # =====================================================\n    # == Section D: Custom Ensemble (ML+LSTM Average Prob) == # <--- Modified Title\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section D: Custom Ensemble (ML+LSTM Average Prob)\\n\" + \"=\"*50) # <--- Modified Title\n\n    # Combine TEST probabilities from available models\n    test_probas_to_average = []\n    valid_model_names_for_avg = []\n\n    # ... (inside Section D) ...\n    test_probas_to_average = []\n    valid_model_names_for_avg = []\n\n    # Check ML Ensemble\n    if \"Ensemble (Soft)\" in ml_probas_test and isinstance(ml_probas_test[\"Ensemble (Soft)\"], np.ndarray) and len(ml_probas_test[\"Ensemble (Soft)\"]) > 0:\n        test_probas_to_average.append(ml_probas_test[\"Ensemble (Soft)\"])\n        valid_model_names_for_avg.append(\"Ensemble (Soft)\")\n        # ...\n    # Check LSTM\n    if \"LSTM\" in dl_probas_test and isinstance(dl_probas_test[\"LSTM\"], np.ndarray) and len(dl_probas_test[\"LSTM\"]) > 0:\n        test_probas_to_average.append(dl_probas_test[\"LSTM\"])\n        valid_model_names_for_avg.append(\"LSTM\")\n        # ...\n\n    # --- This is the crucial check ---\n    if len(test_probas_to_average) >= 2 and len(y_test) > 0:\n        # ... (check lengths match y_test) ...\n        if all(l == len(y_test) for l in lengths):\n             # !!! Only calculate avg_proba_test HERE !!!\n             avg_proba_test = np.mean(np.array(test_probas_to_average), axis=0)\n        else:\n             # Length mismatch case\n             avg_proba_test = None # Stays None\n    else:\n        # Didn't meet conditions case\n        print(\"\\nSkipping custom ensemble (need >= 2 models with valid test probabilities).\")\n        # avg_proba_test stays None here too\n\n    \n\n\n    # =====================================================\n    # == Section E: Saving Artifacts                       ==\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section E: Saving Artifacts\\n\" + \"=\"*50)\n    models_saved = False\n    try:\n        os.makedirs(SAVE_DIR, exist_ok=True)\n        print(f\"Using save directory: {SAVE_DIR}\")\n        models_saved = True\n    except OSError as e:\n        print(f\"❌ Error creating/accessing directory {SAVE_DIR}: {e}.\")\n\n    if models_saved:\n        # Save TF-IDF Vectorizer\n        if vectorizer:\n            try: joblib.dump(vectorizer, os.path.join(SAVE_DIR, \"tfidf_vectorizer.joblib\")); print(f\"✅ Saved vectorizer\")\n            except Exception as e: print(f\"❌ Error saving vectorizer: {e}\")\n        else: print(\"ℹ️ Vectorizer not available, skipping save.\")\n\n        # Save ML Models\n        print(\"\\nSaving ML Models...\")\n        for name, model in ml_models.items():\n            if model is not None:\n                sanitized_name = name.replace(' ', '_').replace('(', '').replace(')', '')\n                try: joblib.dump(model, os.path.join(SAVE_DIR, f\"ml_{sanitized_name}.joblib\")); print(f\"  ✅ Saved ML model '{name}'\")\n                except Exception as e: print(f\"  ❌ Error saving ML model '{name}': {e}\")\n            else: print(f\"  ℹ️ ML Model '{name}' not available, skipping save.\")\n\n        # Save DL Components\n        print(\"\\nSaving DL (LSTM) Components...\")\n        if keras_tokenizer: # Renamed variable\n            try: joblib.dump(keras_tokenizer, os.path.join(SAVE_DIR, \"keras_tokenizer.joblib\")); print(f\"  ✅ Saved Keras tokenizer\") # Renamed variable\n            except Exception as e: print(f\"  ❌ Error saving Keras tokenizer: {e}\")\n        else: print(f\"  ℹ️ Keras Tokenizer not available, skipping save.\")\n\n        if dl_models.get(\"LSTM\"): # Use .get() for safety\n            try: dl_models[\"LSTM\"].save(os.path.join(SAVE_DIR, \"lstm_model.keras\")); print(f\"  ✅ Saved LSTM model\")\n            except Exception as e: print(f\"  ❌ Error saving LSTM model: {e}\")\n        else: print(f\"  ℹ️ LSTM Model not available, skipping save.\")\n\n        # GRU Saving block removed\n\n        # Save Best Thresholds\n        print(\"\\nSaving Best Thresholds...\")\n        if best_thresholds:\n            try: joblib.dump(best_thresholds, os.path.join(SAVE_DIR, \"best_thresholds.joblib\")); print(f\"  ✅ Saved best thresholds\")\n            except Exception as e: print(f\"  ❌ Error saving thresholds: {e}\")\n        else: print(f\"  ℹ️ Best thresholds dictionary empty, skipping save.\")\n\n    else: print(\"\\nArtifact saving skipped.\")\n\n\n    # =====================================================\n    # == Section F: Example Predictions                  ==\n    # =====================================================\n    print(\"\\n\" + \"=\"*50 + \"\\n== Section F: Example Predictions on Test Set\\n\" + \"=\"*50)\n\n    if not test_df.empty and len(y_test) > 0 and len(test_df) >= NUM_TEST_EXAMPLES_TO_SHOW:\n        print(f\"\\nShowing predictions for first {NUM_TEST_EXAMPLES_TO_SHOW} test messages using BEST thresholds:\")\n        example_df = test_df.head(NUM_TEST_EXAMPLES_TO_SHOW)\n        pred_to_label = {0: target_names[0], 1: target_names[1]}\n        example_orig_locs = list(range(NUM_TEST_EXAMPLES_TO_SHOW)) # Indices for head()\n\n        # Pre-fetch probabilities for the examples if available\n        # Use .get() with default empty array to avoid errors if key missing\n        example_ml_probas = {name: ml_probas_test.get(name, np.array([]))[example_orig_locs]\n                             for name in ml_models if ml_probas_test.get(name) is not None and len(ml_probas_test.get(name, [])) > max(example_orig_locs)}\n        example_dl_probas = {name: dl_probas_test.get(name, np.array([]))[example_orig_locs]\n                             for name in dl_models if dl_probas_test.get(name) is not None and len(dl_probas_test.get(name, [])) > max(example_orig_locs)}\n        example_avg_probas = avg_proba_test[example_orig_locs] if avg_proba_test is not None and len(avg_proba_test) > max(example_orig_locs) else None\n\n        for i, index in enumerate(example_df.index):\n            current_loc = example_orig_locs[i]\n            row = example_df.loc[index]\n            message = row['messages']\n            true_label_int = row['labels']\n            true_label_str = pred_to_label[true_label_int]\n\n            print(f\"\\n--- Test Example (Index: {index}, Loc: {current_loc}) ---\")\n            print(f\"  True Label (Sender): {true_label_str}\")\n            print(f\"  Message: '{message[:250]}...'\")\n            print(\"  Model Predictions (using best thresholds):\")\n\n            # ML Predictions\n            for name in ml_models:\n                if name in example_ml_probas and i < len(example_ml_probas[name]):\n                    try:\n                        proba_class1 = example_ml_probas[name][i]\n                        threshold = best_thresholds.get(name, 0.5)\n                        pred_int = (proba_class1 >= threshold).astype(int)\n                        pred_label = pred_to_label.get(pred_int, \"ERR\")\n                        print(f\"    {name:<20}: {pred_label} (Prob: {proba_class1:.3f}, Thr: {threshold:.3f})\")\n                    except Exception as e: print(f\"    {name:<20}: Error getting prediction - {e}\")\n                elif ml_models.get(name) is not None: print(f\"    {name:<20}: Probas N/A\")\n\n            # LSTM Prediction\n            if dl_models.get(\"LSTM\") and \"LSTM\" in example_dl_probas and i < len(example_dl_probas[\"LSTM\"]):\n                 try:\n                     proba_class1 = example_dl_probas[\"LSTM\"][i]\n                     threshold = best_thresholds.get(\"LSTM\", 0.5)\n                     pred_int = (proba_class1 >= threshold).astype(int)\n                     pred_label = pred_to_label.get(pred_int, \"ERR\")\n                     print(f\"    {'LSTM':<20}: {pred_label} (Prob: {proba_class1:.3f}, Thr: {threshold:.3f})\")\n                 except Exception as e: print(f\"    {'LSTM':<20}: Error predicting - {e}\")\n            elif dl_models.get(\"LSTM\"): print(f\"    {'LSTM':<20}: Probas N/A\")\n\n            # GRU Prediction block removed\n\n            # Custom Ensemble Prediction\n            if example_avg_probas is not None and i < len(example_avg_probas):\n                try:\n                    proba_class1 = example_avg_probas[i]\n                    pred_int = (proba_class1 >= avg_threshold).astype(int) # Use threshold calculated in section D\n                    pred_label = pred_to_label.get(pred_int, \"ERR\")\n                    print(f\"    {'CUSTOM ENSEMBLE':<20}: {pred_label} (AvgProb: {proba_class1:.3f}, Thr: {avg_threshold:.3f})\")\n                except Exception as e: print(f\"    {'CUSTOM ENSEMBLE':<20}: Error predicting - {e}\")\n            else: print(f\"    {'CUSTOM ENSEMBLE':<20}: Avg Probas N/A\")\n\n    elif not y_test.size: print(\"\\nTest data is empty, cannot show examples.\")\n    else: print(f\"\\nTest DataFrame has < {NUM_TEST_EXAMPLES_TO_SHOW} samples or other issue.\")\n\n    # =====================================================\n    # == Final Cleanup                                   ==\n    # =====================================================\n    print(\"\\n--- Cleaning up main script memory ---\")\n    del y_train, y_val, y_test\n    del train_texts_processed, val_texts_processed, test_texts_processed\n    if 'df_train' in locals(): del df_train\n    if 'df_val' in locals(): del df_val\n    if 'test_df' in locals(): del test_df\n    if 'example_df' in locals(): del example_df\n    if 'vectorizer' in locals(): del vectorizer\n    if 'ml_models' in locals(): del ml_models\n    if 'dl_models' in locals(): del dl_models # Removes LSTM ref\n    if 'keras_tokenizer' in locals(): del keras_tokenizer\n    # GRU model already removed\n    gc.collect()\n    print(\"\\n--- Full Training Script V3.1 (GRU Removed) Finished ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:37:11.121328Z","iopub.execute_input":"2025-04-15T15:37:11.121647Z","iopub.status.idle":"2025-04-15T15:46:18.488426Z","shell.execute_reply.started":"2025-04-15T15:37:11.121623Z","shell.execute_reply":"2025-04-15T15:46:18.487610Z"}},"outputs":[{"name":"stdout","text":"TensorFlow Version: 2.18.0\n--- Loading Data ---\n✅ Loaded 189 entries from /kaggle/input/nlpdata/train.jsonl.\n✅ Loaded 21 entries from /kaggle/input/nlpdata/validation.jsonl.\n✅ Loaded 42 entries from /kaggle/input/nlpdata/test.jsonl.\n\n--- Preparing DataFrames (using sender_labels) ---\nProcessing data using 'sender_labels' as ground truth...\nDataFrame created with 14548 messages (using sender_labels).\nLabel distribution (sender):\nlabels\n1    0.955527\n0    0.044473\nName: proportion, dtype: float64\nProcessing data using 'sender_labels' as ground truth...\nDataFrame created with 2741 messages (using sender_labels).\nLabel distribution (sender):\nlabels\n1    0.912441\n0    0.087559\nName: proportion, dtype: float64\n\n--- Initial Balancing & Train/Validation Split ---\nInitial Upsampling minority class (0) from 647 to 13901.\nInitially balanced dataset size: 27802\nFinal Train Set Size: 23631, Validation Set Size: 4171\nTrain label distribution:\nlabels\n1    0.500021\n0    0.499979\nName: proportion, dtype: float64\nValidation label distribution:\nlabels\n0    0.50012\n1    0.49988\nName: proportion, dtype: float64\nData shapes: Train Text=23631, Train Labels=(23631,)\n             Val Text=4171, Val Labels=(4171,)\n             Test Text=2741, Test Labels=(2741,)\n\n--- Text Preprocessing ---\nText preprocessing took 0.44 seconds.\n\n==================================================\n== Section A: TF-IDF + ML Models Pipeline\n==================================================\n\n--- A.1 Feature Extraction (TF-IDF) ---\nFitting TF-IDF on training data...\nTransforming validation and test data...\nTF-IDF transformation took 0.72s.\nTF-IDF Shapes: Train=(23631, 5000), Val=(4171, 5000), Test=(2741, 5000)\n\n--- A.2 Applying SMOTE to TF-IDF Training Data ---\nApplying SMOTE...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/imblearn/over_sampling/_smote/base.py:363: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n","output_type":"stream"},{"name":"stdout","text":"SMOTE took 4.76s. Shape: (23632, 5000)\nLabel distribution after SMOTE:\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n\n--- A.3 Training and Evaluating ML Models ---\n\nTraining Logistic Regression...\n✅ Training completed in 0.09 seconds.\n\n--- Evaluation: Logistic Regression @0.5 (Test) (Threshold: 0.500) ---\nAccuracy: 0.8282\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.14      0.19      0.16       240\nDeceptive (1)       0.92      0.89      0.90      2501\n\n     accuracy                           0.83      2741\n    macro avg       0.53      0.54      0.53      2741\n weighted avg       0.85      0.83      0.84      2741\n\n\nKey F1-Scores:\n  F1 Score [Truthful (0)]: 0.1604\n  F1 Score [Deceptive (1)]: 0.9043\n  F1 Score [Macro Avg]:     0.5324\n  F1 Score [Weighted Avg]:  0.8392\n\nConfusion Matrix:\n            Predicted:\n            Truthful (0) Deceptive (1)\nActual Truthful (0) [45           195         ] (TN, FP)\nActual Deceptive (1) [276          2225        ] (FN, TP)\n\nTraining Random Forest...\n✅ Training completed in 4.45 seconds.\n\n--- Evaluation: Random Forest @0.5 (Test) (Threshold: 0.500) ---\nAccuracy: 0.8982\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.05      0.01      0.01       240\nDeceptive (1)       0.91      0.98      0.95      2501\n\n     accuracy                           0.90      2741\n    macro avg       0.48      0.50      0.48      2741\n weighted avg       0.84      0.90      0.86      2741\n\n\nKey F1-Scores:\n  F1 Score [Truthful (0)]: 0.0141\n  F1 Score [Deceptive (1)]: 0.9463\n  F1 Score [Macro Avg]:     0.4802\n  F1 Score [Weighted Avg]:  0.8647\n\nConfusion Matrix:\n            Predicted:\n            Truthful (0) Deceptive (1)\nActual Truthful (0) [2            238         ] (TN, FP)\nActual Deceptive (1) [41           2460        ] (FN, TP)\n\nTraining Multinomial NB...\n✅ Training completed in 0.01 seconds.\n\n--- Evaluation: Multinomial NB @0.5 (Test) (Threshold: 0.500) ---\nAccuracy: 0.8285\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.08      0.09      0.08       240\nDeceptive (1)       0.91      0.90      0.91      2501\n\n     accuracy                           0.83      2741\n    macro avg       0.49      0.49      0.49      2741\n weighted avg       0.84      0.83      0.83      2741\n\n\nKey F1-Scores:\n  F1 Score [Truthful (0)]: 0.0820\n  F1 Score [Deceptive (1)]: 0.9054\n  F1 Score [Macro Avg]:     0.4937\n  F1 Score [Weighted Avg]:  0.8333\n\nConfusion Matrix:\n            Predicted:\n            Truthful (0) Deceptive (1)\nActual Truthful (0) [21           219         ] (TN, FP)\nActual Deceptive (1) [251          2250        ] (FN, TP)\n\n--- A.4 Setting up and Evaluating ML Voting Ensemble ---\nCreating Soft Voting Ensemble with: Logistic Regression, Random Forest, Multinomial NB\nFitting Soft Voting Ensemble...\n❌ Error fitting/evaluating ML Ensemble: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n\n==================================================\n== Section B: Deep Learning Model Pipeline (LSTM Only)\n==================================================\n\n--- B.1 Tokenization & Padding for LSTM ---\nFound 8297 unique tokens in Keras tokenizer.\nKeras Tokenization/Padding took 0.82s.\nPadded Shapes: Train=(23631, 100), Val=(4171, 100), Test=(2741, 100)\n\n--- B.2 Preparing GloVe Embedding Matrix for LSTM ---\nLoading GloVe embeddings from: /kaggle/input/nlpdata/glove.6B.100d.txt\n✅ Found 400000 word vectors with dim 100.\nEmbedding matrix created in 7.98s. 7103 hits, 1194 misses.\n\n--- B.3 Calculating Class Weights & Setting Early Stopping ---\nCalculated DL Class Weights: {0: 1.0000423190859078, 1: 0.9999576844955992}\n\n--- B.4 Training LSTM Model ---\nBuilding LSTM Model...\nUsing pre-trained GloVe embeddings (fine-tuning enabled).\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"LSTM_Model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_Model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m1,000,000\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ spatial_dropout1d_3                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ spatial_dropout1d_3                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 229ms/step - accuracy: 0.5668 - loss: 0.6777 - precision_truthful: 0.5692 - recall_truthful: 0.5850 - val_accuracy: 0.6310 - val_loss: 0.6343 - val_precision_truthful: 0.6529 - val_recall_truthful: 0.5592\nEpoch 2/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 226ms/step - accuracy: 0.6471 - loss: 0.6243 - precision_truthful: 0.6483 - recall_truthful: 0.6343 - val_accuracy: 0.7387 - val_loss: 0.5141 - val_precision_truthful: 0.7516 - val_recall_truthful: 0.7127\nEpoch 3/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 225ms/step - accuracy: 0.7438 - loss: 0.5170 - precision_truthful: 0.7514 - recall_truthful: 0.7285 - val_accuracy: 0.8655 - val_loss: 0.3293 - val_precision_truthful: 0.9015 - val_recall_truthful: 0.8206\nEpoch 4/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 224ms/step - accuracy: 0.8299 - loss: 0.3794 - precision_truthful: 0.8416 - recall_truthful: 0.8089 - val_accuracy: 0.9027 - val_loss: 0.2407 - val_precision_truthful: 0.9393 - val_recall_truthful: 0.8609\nEpoch 5/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 226ms/step - accuracy: 0.8734 - loss: 0.2965 - precision_truthful: 0.8859 - recall_truthful: 0.8563 - val_accuracy: 0.9146 - val_loss: 0.2093 - val_precision_truthful: 0.9742 - val_recall_truthful: 0.8518\nEpoch 6/6\n\u001b[1m370/370\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 227ms/step - accuracy: 0.8957 - loss: 0.2508 - precision_truthful: 0.9104 - recall_truthful: 0.8736 - val_accuracy: 0.9314 - val_loss: 0.1636 - val_precision_truthful: 0.9687 - val_recall_truthful: 0.8916\nRestoring model weights from the end of the best epoch: 6.\n✅ LSTM Training completed in 509.09 seconds (stopped epoch: Not stopped early).\n\nEvaluating LSTM on Test Set...\nLSTM Prediction took 4.91s.\n\n--- Evaluation: LSTM @0.5 (Test) (Threshold: 0.500) ---\nAccuracy: 0.8267\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.16      0.22      0.18       240\nDeceptive (1)       0.92      0.88      0.90      2501\n\n     accuracy                           0.83      2741\n    macro avg       0.54      0.55      0.54      2741\n weighted avg       0.85      0.83      0.84      2741\n\n\nKey F1-Scores:\n  F1 Score [Truthful (0)]: 0.1824\n  F1 Score [Deceptive (1)]: 0.9031\n  F1 Score [Macro Avg]:     0.5428\n  F1 Score [Weighted Avg]:  0.8400\n\nConfusion Matrix:\n            Predicted:\n            Truthful (0) Deceptive (1)\nActual Truthful (0) [53           187         ] (TN, FP)\nActual Deceptive (1) [288          2213        ] (FN, TP)\nGetting LSTM probabilities on Validation Set...\n\n==================================================\n== Section C: Threshold Tuning (using Validation Set)\n==================================================\nFinding best thresholds for models: ['Logistic Regression', 'Random Forest', 'Multinomial NB', 'Ensemble (Soft)', 'LSTM']\n\nTuning threshold for Logistic Regression...\nFinding best threshold for class 0 (Truthful (0)) using F1 score...\nBest threshold found: 0.460 (Maximizes F1 for class 0 at 0.9277)\n\nTuning threshold for Random Forest...\nFinding best threshold for class 0 (Truthful (0)) using F1 score...\nBest threshold found: 0.490 (Maximizes F1 for class 0 at 0.9841)\n\nTuning threshold for Multinomial NB...\nFinding best threshold for class 0 (Truthful (0)) using F1 score...\nBest threshold found: 0.580 (Maximizes F1 for class 0 at 0.8362)\nSkipping threshold tuning for 'Ensemble (Soft)' (no valid probabilities).\n\nTuning threshold for LSTM...\nFinding best threshold for class 0 (Truthful (0)) using F1 score...\nBest threshold found: 0.230 (Maximizes F1 for class 0 at 0.9459)\n\nBest Thresholds found (optimizing F1 for Truthful class on Validation Set):\n  Logistic Regression      : 0.460\n  Random Forest            : 0.490\n  Multinomial NB           : 0.580\n  Ensemble (Soft)          : 0.500\n  LSTM                     : 0.230\n\n==================================================\n== Section D: Custom Ensemble (ML+LSTM Average Prob)\n==================================================\n\nSkipping custom ensemble (need >= 2 models with valid test probabilities).\n\n==================================================\n== Section E: Saving Artifacts\n==================================================\nUsing save directory: trained_models_v3.1_smote_lstm_ensemble\n✅ Saved vectorizer\n\nSaving ML Models...\n  ✅ Saved ML model 'Logistic Regression'\n  ✅ Saved ML model 'Random Forest'\n  ✅ Saved ML model 'Multinomial NB'\n  ℹ️ ML Model 'Ensemble (Soft)' not available, skipping save.\n\nSaving DL (LSTM) Components...\n  ✅ Saved Keras tokenizer\n  ✅ Saved LSTM model\n\nSaving Best Thresholds...\n  ✅ Saved best thresholds\n\n==================================================\n== Section F: Example Predictions on Test Set\n==================================================\n\nShowing predictions for first 5 test messages using BEST thresholds:\n\n--- Test Example (Index: 0, Loc: 0) ---\n  True Label (Sender): Deceptive (1)\n  Message: 'Hi Italy! Just opening up communication, and I want to know what some of your initial thoughts on the game are and if/how we can work together...'\n  Model Predictions (using best thresholds):\n    Logistic Regression : Deceptive (1) (Prob: 0.949, Thr: 0.460)\n    Random Forest       : Deceptive (1) (Prob: 0.940, Thr: 0.490)\n    Multinomial NB      : Deceptive (1) (Prob: 0.893, Thr: 0.580)\n    LSTM                : Deceptive (1) (Prob: 0.995, Thr: 0.230)\n    CUSTOM ENSEMBLE     : Avg Probas N/A\n\n--- Test Example (Index: 1, Loc: 1) ---\n  True Label (Sender): Deceptive (1)\n  Message: 'Well....that's a great question, and a lot of it comes down to how free I'll be left to play in the West, no?...'\n  Model Predictions (using best thresholds):\n    Logistic Regression : Deceptive (1) (Prob: 0.909, Thr: 0.460)\n    Random Forest       : Deceptive (1) (Prob: 0.960, Thr: 0.490)\n    Multinomial NB      : Deceptive (1) (Prob: 0.928, Thr: 0.580)\n    LSTM                : Deceptive (1) (Prob: 1.000, Thr: 0.230)\n    CUSTOM ENSEMBLE     : Avg Probas N/A\n\n--- Test Example (Index: 2, Loc: 2) ---\n  True Label (Sender): Deceptive (1)\n  Message: 'Well, if you want to attack France in the Mediterranean while I attack through Burgundy you can have Marseille and Iberia while I take Brest and Paris, then with France out of the way you could focus on Turkey or Austria. Sound fair?...'\n  Model Predictions (using best thresholds):\n    Logistic Regression : Deceptive (1) (Prob: 0.551, Thr: 0.460)\n    Random Forest       : Deceptive (1) (Prob: 0.920, Thr: 0.490)\n    Multinomial NB      : Deceptive (1) (Prob: 0.638, Thr: 0.580)\n    LSTM                : Truthful (0) (Prob: 0.206, Thr: 0.230)\n    CUSTOM ENSEMBLE     : Avg Probas N/A\n\n--- Test Example (Index: 3, Loc: 3) ---\n  True Label (Sender): Deceptive (1)\n  Message: 'Hello, I'm just asking about your move to Tyrolia. It's making me more than a little concerned...'\n  Model Predictions (using best thresholds):\n    Logistic Regression : Deceptive (1) (Prob: 0.614, Thr: 0.460)\n    Random Forest       : Deceptive (1) (Prob: 0.920, Thr: 0.490)\n    Multinomial NB      : Deceptive (1) (Prob: 0.773, Thr: 0.580)\n    LSTM                : Deceptive (1) (Prob: 0.959, Thr: 0.230)\n    CUSTOM ENSEMBLE     : Avg Probas N/A\n\n--- Test Example (Index: 4, Loc: 4) ---\n  True Label (Sender): Deceptive (1)\n  Message: 'Totally understandable - but did you notice the attempt at Trieste?  Tyrolia is the natural support position for that attempt 🙂...'\n  Model Predictions (using best thresholds):\n    Logistic Regression : Deceptive (1) (Prob: 0.559, Thr: 0.460)\n    Random Forest       : Deceptive (1) (Prob: 0.940, Thr: 0.490)\n    Multinomial NB      : Deceptive (1) (Prob: 0.663, Thr: 0.580)\n    LSTM                : Deceptive (1) (Prob: 1.000, Thr: 0.230)\n    CUSTOM ENSEMBLE     : Avg Probas N/A\n\n--- Cleaning up main script memory ---\n\n--- Full Training Script V3.1 (GRU Removed) Finished ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nDiplomacy Message Classification - Inference Script for V3.1 Models\nLoads artifacts from V3.1 training (TF-IDF/ML, LSTM) and predicts on the test set.\nCalculates overall evaluation metrics and shows examples based on TRUE labels.\n\n**MODIFIED: Example selection now based on TRUE labels, not predicted labels.**\n\"\"\"\n\nimport json\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport time\nimport joblib\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport sys\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='joblib')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TensorFlow INFO messages\n\n# ML Imports\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\nfrom sklearn.exceptions import NotFittedError\n\n# DL Imports\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import load_model as load_keras_model\n\n# --- Configuration ---\n# !!! MUST MATCH the V3.1 training script outputs !!!\nSAVE_DIR = \"trained_models_v3.1_smote_lstm_ensemble\" # <--- CHECK/SET THIS PATH !!!\nTEST_FILE_PATH = \"/kaggle/input/nlpdata/test.jsonl\" # <--- CHECK/SET THIS PATH (Test data to evaluate)\n\n# Parameters needed for loading/padding (should match V3.1 training)\nDL_MAX_LENGTH = 100\n\n# Target names for output labels\nTARGET_NAMES = [\"Truthful (0)\", \"Deceptive (1)\"]\nNUM_EXAMPLES_TO_SHOW = 5\n\n# --- Preprocessing Function (identical to training V3.1) ---\ntry:\n    STOPWORDS = set(stopwords.words('english'))\nexcept LookupError:\n    print(\"NLTK stopwords not found. Downloading...\")\n    nltk.download('stopwords', quiet=True)\n    STOPWORDS = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    \"\"\"Basic text preprocessing.\"\"\"\n    if not isinstance(text, str): text = str(text)\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# --- Helper Functions (Copied from Training Script) ---\ndef load_jsonl(file_path):\n    \"\"\"Loads data from a JSON Lines file.\"\"\"\n    data = []\n    if not os.path.exists(file_path):\n        base_name = os.path.basename(file_path); dir_name = os.path.basename(os.path.dirname(file_path))\n        kg1=f\"/kaggle/input/{base_name}\"; kg2=f\"/kaggle/input/{dir_name}/{base_name}\"; print(f\"File not found: '{file_path}'. Trying Kaggle paths...\")\n        if os.path.exists(kg1): print(f\"Found: '{kg1}'\"); file_path = kg1\n        elif os.path.exists(kg2): print(f\"Found: '{kg2}'\"); file_path = kg2\n        else: raise FileNotFoundError(f\"❌ Data file not found: '{file_path}' or Kaggle paths.\")\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for ln, line in enumerate(f):\n                try: data.append(json.loads(line))\n                except json.JSONDecodeError as e: print(f\"Warn: Skip JSON line {ln+1}: {e}\")\n        print(f\"✅ Loaded {len(data)} entries from {file_path}.\")\n        return data\n    except Exception as e: raise RuntimeError(f\"❌ Error loading {file_path}: {e}\")\n\ndef data_to_dataframe(jsonl_data):\n    \"\"\"Converts loaded JSONL data into a message DataFrame using SENDER_LABELS.\"\"\"\n    msgs, labels, gids, ridxs = [], [], [], []\n    print(\"Processing data using 'sender_labels'...\")\n    for i, gdata in enumerate(jsonl_data):\n        gid = gdata.get(\"game_id\", f\"UNK_{i}\")\n        if not isinstance(gdata, dict) or \"messages\" not in gdata or \"sender_labels\" not in gdata: print(f\"Warn: Skip game {i}: Invalid data.\"); continue\n        m, sl = gdata[\"messages\"], gdata[\"sender_labels\"]\n        if not isinstance(m,list) or not isinstance(sl,list): print(f\"Warn: Game {gid} msgs/labels not lists.\"); continue\n        n_m, n_l = len(m), len(sl)\n        if n_m != n_l: print(f\"Warn: Game {gid} mismatch ({n_m}/{n_l}).\"); continue\n        ri = gdata.get(\"relative_message_index\", list(range(n_m)))\n        if not isinstance(ri, list) or len(ri)!=n_m: print(f\"Warn: Game {gid} bad rel_idx.\"); ri = list(range(n_m))\n        msgs.extend(m); labels.extend(sl); gids.extend([gid]*n_m); ridxs.extend(ri)\n    if not msgs: print(\"Warn: No valid messages found.\"); return pd.DataFrame()\n    df = pd.DataFrame({\"game_id\": gids, \"relative_index\": ridxs, \"messages\": msgs, \"labels\": labels})\n    df['labels'] = df['labels'].apply(lambda x: 1 if str(x).lower()=='true' else 0)\n    print(f\"DataFrame created with {len(df)} messages.\")\n    if not df.empty: print(f\"Label distribution:\\n{df['labels'].value_counts(normalize=True)}\")\n    return df\n\ndef print_evaluation_metrics(y_true, y_pred, model_name, target_names, threshold=0.5):\n    \"\"\"Calculates and prints detailed classification metrics using a specific threshold.\"\"\"\n    print(f\"\\n--- Evaluation: {model_name} (Threshold Used: {threshold:.3f}) ---\") # Clarified threshold is the one used for prediction\n    y_t, y_p = np.asarray(y_true), np.asarray(y_pred)\n    if len(y_t)!=len(y_p) or len(y_t)==0: print(\"Error: Invalid labels/preds.\"); return {\"acc\":0.0,\"f1_T\":0.0,\"f1_D\":0.0,\"f1_M\":0.0}\n    acc = accuracy_score(y_t, y_p); print(f\"Accuracy: {acc:.4f}\")\n    print(\"\\nClassification Report:\")\n    try:\n        cr_txt = classification_report(y_t, y_p, labels=[0,1], target_names=target_names, zero_division=0); print(cr_txt)\n        cr_dict = classification_report(y_t, y_p, labels=[0,1], target_names=target_names, zero_division=0, output_dict=True)\n        f1_t = cr_dict.get(target_names[0],{}).get('f1-score',0.0); f1_d = cr_dict.get(target_names[1],{}).get('f1-score',0.0)\n        f1_m = cr_dict.get('macro avg',{}).get('f1-score',0.0); f1_w = cr_dict.get('weighted avg',{}).get('f1-score',0.0)\n        print(\"\\nKey F1-Scores:\"); print(f\" F1[{target_names[0]}]: {f1_t:.4f}\"); print(f\" F1[{target_names[1]}]: {f1_d:.4f}\"); print(f\" F1[Macro]: {f1_m:.4f}\"); print(f\" F1[Weighted]: {f1_w:.4f}\")\n        print(\"\\nConfusion Matrix:\"); cm = confusion_matrix(y_t, y_p, labels=[0,1]); tn, fp, fn, tp = cm.ravel()\n        print(f\"      Pred:{target_names[0]:<12}{target_names[1]:<12}\"); print(f\"Act {target_names[0]:<5} [{tn:<12}{fp:<12}]\"); print(f\"Act {target_names[1]:<5} [{fn:<12}{tp:<12}]\")\n        return {\"accuracy\":acc, \"report\":cr_dict, \"f1_truthful\":f1_t, \"f1_deceptive\":f1_d, \"f1_macro\":f1_m}\n    except Exception as e: print(f\"❌ Metrics Error: {e}\"); return {\"accuracy\":acc,\"report\":\"Error\"}\n\n\n# --- Artifact Loading Function (V3.1 specific) ---\ndef load_artifacts_v3_1(save_dir):\n    \"\"\"Loads V3.1 artifacts (ML, LSTM) from the specified directory.\"\"\"\n    print(f\"\\n--- Loading Artifacts (V3.1) from: {save_dir} ---\")\n    artifacts = {\n        \"tfidf_vectorizer\": None, \"ml_models\": {}, \"keras_tokenizer\": None,\n        \"lstm_model\": None, \"best_thresholds\": None\n    }\n    critical_load_ok = True\n\n    # 1. TF-IDF Vectorizer\n    try:\n        vectorizer_path = os.path.join(save_dir, \"tfidf_vectorizer.joblib\")\n        artifacts[\"tfidf_vectorizer\"] = joblib.load(vectorizer_path); print(f\"  (+) TF-IDF Vectorizer\")\n    except Exception as e: print(f\"  (-) Error loading TF-IDF Vectorizer: {e}\"); critical_load_ok = False\n\n    # 2. ML Models\n    ml_model_keys = [\"Logistic Regression\", \"Random Forest\", \"Multinomial NB\", \"Ensemble (Soft)\"]\n    for name in ml_model_keys:\n        sanitized_name = name.replace(' ', '_').replace('(', '').replace(')', '')\n        try:\n            model_path = os.path.join(save_dir, f\"ml_{sanitized_name}.joblib\")\n            artifacts[\"ml_models\"][name] = joblib.load(model_path); print(f\"  (+) ML Model: {name}\")\n        except FileNotFoundError: print(f\"  (-) Info: ML Model '{name}' not found.\")\n        except Exception as e: print(f\"  (-) Warn: Loading ML Model '{name}' failed: {e}\")\n\n    # 3. Keras Tokenizer\n    try:\n        keras_tok_path = os.path.join(save_dir, \"keras_tokenizer.joblib\") # Matches corrected V3.1 saving\n        artifacts[\"keras_tokenizer\"] = joblib.load(keras_tok_path); print(f\"  (+) Keras Tokenizer\")\n    except Exception as e: print(f\"  (-) Error loading Keras Tokenizer: {e}\"); critical_load_ok = False\n\n    # 4. LSTM Model\n    try:\n        lstm_path = os.path.join(save_dir, \"lstm_model.keras\")\n        artifacts[\"lstm_model\"] = load_keras_model(lstm_path); print(f\"  (+) LSTM Model\")\n    except Exception as e: print(f\"  (-) Warn: Loading LSTM Model failed: {e}\")\n\n    # 5. Best Thresholds\n    try:\n        thresh_path = os.path.join(save_dir, \"best_thresholds.joblib\")\n        artifacts[\"best_thresholds\"] = joblib.load(thresh_path); print(f\"  (+) Best Thresholds: {artifacts['best_thresholds']}\")\n    except Exception as e: print(f\"  (-) Error loading Best Thresholds: {e}. Using default 0.5.\"); critical_load_ok = False; artifacts[\"best_thresholds\"] = {}\n\n    print(\"--- Artifact Loading Complete ---\")\n    if not critical_load_ok: print(\"\\n❌ Error: One or more critical artifacts failed to load.\")\n    return artifacts, critical_load_ok\n\n# === Main Inference Execution ===\nif __name__ == \"__main__\":\n\n    print(f\"Starting V3.1 Inference Script...\")\n    print(f\"Python Executable: {sys.executable}\")\n\n    # --- Load Artifacts ---\n    artifacts, artifacts_ok = load_artifacts_v3_1(SAVE_DIR)\n    if not artifacts_ok:\n        print(\"\\nExiting due to critical artifact loading failure.\")\n        sys.exit(1)\n\n    # --- Load Test Data ---\n    print(\"\\n--- Loading Test Data ---\")\n    try:\n        test_data = load_jsonl(TEST_FILE_PATH)\n        test_df = data_to_dataframe(test_data)\n        if test_df.empty: raise ValueError(\"Test DataFrame is empty after loading.\")\n        test_texts_raw = test_df['messages'].tolist()\n        y_test = np.array(test_df['labels'].tolist()) # True labels\n        print(f\"Loaded {len(test_texts_raw)} test messages.\")\n    except (FileNotFoundError, RuntimeError, ValueError) as e:\n        print(f\"❌ Error loading or processing test data from {TEST_FILE_PATH}: {e}\")\n        print(\"Cannot perform evaluation. Exiting.\")\n        sys.exit(1)\n    del test_data # Free memory\n    gc.collect()\n\n    # --- Preprocess Test Data ---\n    print(\"\\n--- Preprocessing Test Data ---\")\n    start_time = time.time()\n    test_texts_processed = [preprocess_text(text) for text in test_texts_raw]\n    print(f\"Preprocessing took {time.time() - start_time:.2f}s.\")\n\n    # --- Feature Extraction for Test Data ---\n    print(\"\\n--- Extracting Features for Test Data ---\")\n    X_test_tfidf = None\n    if artifacts[\"tfidf_vectorizer\"]:\n        try:\n            X_test_tfidf = artifacts[\"tfidf_vectorizer\"].transform(test_texts_processed)\n            print(f\"  TF-IDF features generated. Shape: {X_test_tfidf.shape}\")\n        except Exception as e: print(f\"  ❌ Error generating TF-IDF features: {e}\")\n    else: print(\"  Skipping TF-IDF (vectorizer not loaded).\")\n\n    X_test_keras_padded = None\n    if artifacts[\"keras_tokenizer\"]:\n        try:\n            test_sequences = artifacts[\"keras_tokenizer\"].texts_to_sequences(test_texts_processed)\n            X_test_keras_padded = pad_sequences(test_sequences, maxlen=DL_MAX_LENGTH, padding='post', truncating='post')\n            print(f\"  Keras padded sequences generated. Shape: {X_test_keras_padded.shape}\")\n        except Exception as e: print(f\"  ❌ Error generating Keras sequences: {e}\")\n    else: print(\"  Skipping Keras padding (tokenizer not loaded).\")\n\n\n    # --- Generate Predictions on Full Test Set ---\n    print(\"\\n\" + \"=\"*50 + \"\\n== Generating Predictions for Full Test Set ==\\n\" + \"=\"*50)\n    all_test_preds = {}\n    all_test_probas = {} # Store class 1 probabilities\n    model_thresholds = artifacts.get(\"best_thresholds\", {})\n\n    # ML Model Predictions\n    if X_test_tfidf is not None:\n        for name, model in artifacts[\"ml_models\"].items():\n            if model:\n                try:\n                    print(f\"Predicting with {name}...\")\n                    if hasattr(model, \"predict_proba\"):\n                        probas = model.predict_proba(X_test_tfidf)[:, 1]\n                        thresh = model_thresholds.get(name, 0.5) # Use tuned threshold or default\n                        preds = (probas >= thresh).astype(int)\n                        all_test_probas[name] = probas\n                        all_test_preds[name] = preds\n                    else:\n                        preds = model.predict(X_test_tfidf)\n                        all_test_preds[name] = preds\n                        all_test_probas[name] = np.array([np.nan]*len(preds)) # Indicate no probability\n                        print(f\"  (Used predict(), no probabilities for soft ensemble from {name})\")\n                except Exception as e: print(f\"  ❌ Error predicting with {name}: {e}\")\n            else: print(f\"  Skipping {name} (not loaded).\")\n\n    # LSTM Model Prediction\n    if X_test_keras_padded is not None and artifacts[\"lstm_model\"]:\n        try:\n            print(f\"Predicting with LSTM...\")\n            probas = artifacts[\"lstm_model\"].predict(X_test_keras_padded, verbose=0).flatten()\n            thresh = model_thresholds.get(\"LSTM\", 0.5)\n            preds = (probas >= thresh).astype(int)\n            all_test_probas[\"LSTM\"] = probas\n            all_test_preds[\"LSTM\"] = preds\n        except Exception as e: print(f\"  ❌ Error predicting with LSTM: {e}\")\n\n    # Calculate Ensemble Prediction\n    print(\"\\nCalculating Ensemble Prediction...\")\n    valid_probas_list = []\n    valid_model_names_for_avg = []\n    # Define models eligible for probability averaging\n    # Prioritize the VotingClassifier if it exists and has probabilities\n    if \"Ensemble (Soft)\" in all_test_probas and all_test_probas[\"Ensemble (Soft)\"] is not None and not np.isnan(all_test_probas[\"Ensemble (Soft)\"]).any():\n         valid_probas_list.append(all_test_probas[\"Ensemble (Soft)\"])\n         valid_model_names_for_avg.append(\"Ensemble (Soft)\")\n    # If not, use base ML models that have probabilities\n    elif not artifacts[\"ml_models\"].get(\"Ensemble (Soft)\"):\n         for name in [\"Logistic Regression\", \"Random Forest\", \"Multinomial NB\"]:\n             if name in all_test_probas and all_test_probas[name] is not None and not np.isnan(all_test_probas[name]).any() and artifacts[\"ml_models\"].get(name):\n                 valid_probas_list.append(all_test_probas[name])\n                 valid_model_names_for_avg.append(name)\n\n    # Add LSTM if its probabilities are valid\n    if \"LSTM\" in all_test_probas and all_test_probas[\"LSTM\"] is not None and not np.isnan(all_test_probas[\"LSTM\"]).any():\n         valid_probas_list.append(all_test_probas[\"LSTM\"])\n         valid_model_names_for_avg.append(\"LSTM\")\n\n    final_avg_threshold = 0.5 # Default\n    if len(valid_probas_list) >= 2:\n        print(f\"  Ensemble based on: {', '.join(valid_model_names_for_avg)}\")\n        # Ensure all arrays in list have the same length before stacking\n        first_len = len(valid_probas_list[0])\n        if all(len(arr) == first_len for arr in valid_probas_list):\n            avg_proba_test_full = np.mean(np.stack(valid_probas_list, axis=0), axis=0) # Use np.stack\n            relevant_thresholds = [model_thresholds.get(name, 0.5) for name in valid_model_names_for_avg]\n            final_avg_threshold = np.mean(relevant_thresholds)\n            ensemble_pred_full = (avg_proba_test_full >= final_avg_threshold).astype(int)\n            all_test_probas[\"ENSEMBLE\"] = avg_proba_test_full\n            all_test_preds[\"ENSEMBLE\"] = ensemble_pred_full\n            print(f\"  Ensemble calculated using average threshold: {final_avg_threshold:.3f}\")\n        else:\n            print(\"  ❌ Error: Probability arrays have inconsistent lengths. Cannot calculate ensemble.\")\n            all_test_probas[\"ENSEMBLE\"] = None\n            all_test_preds[\"ENSEMBLE\"] = None\n    elif len(valid_probas_list) == 1:\n        print(f\"  Only one model ({valid_model_names_for_avg[0]}) produced valid probabilities. Using its prediction as 'ensemble'.\")\n        single_model_name = valid_model_names_for_avg[0]\n        all_test_probas[\"ENSEMBLE\"] = all_test_probas[single_model_name]\n        final_avg_threshold = model_thresholds.get(single_model_name, 0.5)\n        all_test_preds[\"ENSEMBLE\"] = (all_test_probas[\"ENSEMBLE\"] >= final_avg_threshold).astype(int)\n    else:\n        print(\"  Skipping Ensemble calculation (fewer than 2 valid model probabilities).\")\n        all_test_probas[\"ENSEMBLE\"] = None\n        all_test_preds[\"ENSEMBLE\"] = None\n\n\n    # --- Overall Evaluation Metrics ---\n    print(\"\\n\\n\" + \"=\"*50 + \"\\n== Overall Test Set Evaluation Metrics ==\\n\" + \"=\"*50)\n    evaluation_results = {}\n    # Define the order for printing evaluations\n    eval_order = [\"Logistic Regression\", \"Random Forest\", \"Multinomial NB\", \"Ensemble (Soft)\", \"LSTM\", \"ENSEMBLE\"]\n\n    for name in eval_order:\n        preds = all_test_preds.get(name) # Use .get() to handle missing keys safely\n        if preds is not None:\n            threshold_used = 0.5 # Default\n            if name == \"ENSEMBLE\":\n                 threshold_used = final_avg_threshold # Use calculated ensemble threshold\n            elif name in model_thresholds:\n                 threshold_used = model_thresholds[name] # Use specific model threshold\n\n            evaluation_results[name] = print_evaluation_metrics(\n                y_test,\n                preds,\n                model_name=f\"{name} (Test Set)\",\n                target_names=TARGET_NAMES,\n                threshold=threshold_used\n            )\n        else:\n            print(f\"\\n--- Evaluation: {name} (Test Set) ---\")\n            print(\"  Skipped (predictions not available).\")\n\n\n    # --- Display Example Predictions based on TRUE Label --- ## MODIFIED SECTION ##\n    print(\"\\n\\n\" + \"=\"*60 + \"\\n== Example Predictions by ENSEMBLE (Selected by True Label) ==\\n\" + \"=\"*60)\n\n    if all_test_preds.get(\"ENSEMBLE\") is not None:\n        # Create DataFrame for easy filtering\n        results_df = pd.DataFrame({\n            'Original_Text': test_texts_raw,\n            'True_Label_Int': y_test,\n            'Ensemble_Pred_Int': all_test_preds[\"ENSEMBLE\"]\n        })\n        # Add string labels for easier reading\n        results_df['True_Label'] = results_df['True_Label_Int'].map({0: TARGET_NAMES[0], 1: TARGET_NAMES[1]})\n        results_df['Ensemble_Pred'] = results_df['Ensemble_Pred_Int'].map({0: TARGET_NAMES[0], 1: TARGET_NAMES[1]})\n\n        # Examples where TRUE label is Truthful (0)\n        print(f\"\\n--- {NUM_EXAMPLES_TO_SHOW} Examples with TRUE Label = TRUTHFUL ---\")\n        truthful_examples = results_df[results_df['True_Label_Int'] == 0].head(NUM_EXAMPLES_TO_SHOW)\n        if truthful_examples.empty:\n            print(\"No examples found with true label Truthful in the test set.\")\n        else:\n            for i, row in truthful_examples.iterrows(): # Use original index i\n                print(f\"\\nExample (Index {i}):\")\n                print(f\"  True Label:      {row['True_Label']}\")\n                print(f\"  Ensemble Pred:   {row['Ensemble_Pred']}\") # Show what the ensemble predicted\n                print(f\"  Original Text:   '{row['Original_Text'][:300]}...'\")\n\n        # Examples where TRUE label is Deceptive (1)\n        print(f\"\\n--- {NUM_EXAMPLES_TO_SHOW} Examples with TRUE Label = DECEPTIVE ---\")\n        deceptive_examples = results_df[results_df['True_Label_Int'] == 1].head(NUM_EXAMPLES_TO_SHOW)\n        if deceptive_examples.empty:\n            print(\"No examples found with true label Deceptive in the test set.\")\n        else:\n            for i, row in deceptive_examples.iterrows(): # Use original index i\n                print(f\"\\nExample (Index {i}):\")\n                print(f\"  True Label:      {row['True_Label']}\")\n                print(f\"  Ensemble Pred:   {row['Ensemble_Pred']}\") # Show what the ensemble predicted\n                print(f\"  Original Text:   '{row['Original_Text'][:300]}...'\")\n\n    else:\n        print(\"\\nCannot show examples because Ensemble predictions were not generated.\")\n\n\n    # --- Final Cleanup ---\n    print(\"\\n--- Cleaning up inference memory ---\")\n    del artifacts # Remove loaded artifacts\n    if 'test_df' in locals(): del test_df\n    if 'results_df' in locals(): del results_df\n    if 'X_test_tfidf' in locals(): del X_test_tfidf\n    if 'X_test_keras_padded' in locals(): del X_test_keras_padded\n    gc.collect()\n    print(\"--- V3.1 Inference Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:49:28.045197Z","iopub.execute_input":"2025-04-15T15:49:28.045774Z","iopub.status.idle":"2025-04-15T15:49:35.020906Z","shell.execute_reply.started":"2025-04-15T15:49:28.045743Z","shell.execute_reply":"2025-04-15T15:49:35.020338Z"}},"outputs":[{"name":"stdout","text":"Starting V3.1 Inference Script...\nPython Executable: /usr/bin/python3\n\n--- Loading Artifacts (V3.1) from: trained_models_v3.1_smote_lstm_ensemble ---\n  (+) TF-IDF Vectorizer\n  (+) ML Model: Logistic Regression\n  (+) ML Model: Random Forest\n  (+) ML Model: Multinomial NB\n  (-) Info: ML Model 'Ensemble (Soft)' not found.\n  (+) Keras Tokenizer\n  (+) LSTM Model\n  (+) Best Thresholds: {'Logistic Regression': 0.46, 'Random Forest': 0.49, 'Multinomial NB': 0.5800000000000001, 'Ensemble (Soft)': 0.5, 'LSTM': 0.23}\n--- Artifact Loading Complete ---\n\n--- Loading Test Data ---\n✅ Loaded 42 entries from /kaggle/input/nlpdata/test.jsonl.\nProcessing data using 'sender_labels'...\nDataFrame created with 2741 messages.\nLabel distribution:\nlabels\n1    0.912441\n0    0.087559\nName: proportion, dtype: float64\nLoaded 2741 test messages.\n\n--- Preprocessing Test Data ---\nPreprocessing took 0.04s.\n\n--- Extracting Features for Test Data ---\n  TF-IDF features generated. Shape: (2741, 5000)\n  Keras padded sequences generated. Shape: (2741, 100)\n\n==================================================\n== Generating Predictions for Full Test Set ==\n==================================================\nPredicting with Logistic Regression...\nPredicting with Random Forest...\nPredicting with Multinomial NB...\nPredicting with LSTM...\n\nCalculating Ensemble Prediction...\n  Ensemble based on: Logistic Regression, Random Forest, Multinomial NB, LSTM\n  Ensemble calculated using average threshold: 0.440\n\n\n==================================================\n== Overall Test Set Evaluation Metrics ==\n==================================================\n\n--- Evaluation: Logistic Regression (Test Set) (Threshold Used: 0.460) ---\nAccuracy: 0.8497\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.15      0.16      0.16       240\nDeceptive (1)       0.92      0.92      0.92      2501\n\n     accuracy                           0.85      2741\n    macro avg       0.54      0.54      0.54      2741\n weighted avg       0.85      0.85      0.85      2741\n\n\nKey F1-Scores:\n F1[Truthful (0)]: 0.1557\n F1[Deceptive (1)]: 0.9175\n F1[Macro]: 0.5366\n F1[Weighted]: 0.8508\n\nConfusion Matrix:\n      Pred:Truthful (0)Deceptive (1)\nAct Truthful (0) [38          202         ]\nAct Deceptive (1) [210         2291        ]\n\n--- Evaluation: Random Forest (Test Set) (Threshold Used: 0.490) ---\nAccuracy: 0.8993\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.05      0.01      0.01       240\nDeceptive (1)       0.91      0.98      0.95      2501\n\n     accuracy                           0.90      2741\n    macro avg       0.48      0.50      0.48      2741\n weighted avg       0.84      0.90      0.87      2741\n\n\nKey F1-Scores:\n F1[Truthful (0)]: 0.0143\n F1[Deceptive (1)]: 0.9469\n F1[Macro]: 0.4806\n F1[Weighted]: 0.8653\n\nConfusion Matrix:\n      Pred:Truthful (0)Deceptive (1)\nAct Truthful (0) [2           238         ]\nAct Deceptive (1) [38          2463        ]\n\n--- Evaluation: Multinomial NB (Test Set) (Threshold Used: 0.580) ---\nAccuracy: 0.7771\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.09      0.16      0.11       240\nDeceptive (1)       0.91      0.84      0.87      2501\n\n     accuracy                           0.78      2741\n    macro avg       0.50      0.50      0.49      2741\n weighted avg       0.84      0.78      0.81      2741\n\n\nKey F1-Scores:\n F1[Truthful (0)]: 0.1106\n F1[Deceptive (1)]: 0.8726\n F1[Macro]: 0.4916\n F1[Weighted]: 0.8059\n\nConfusion Matrix:\n      Pred:Truthful (0)Deceptive (1)\nAct Truthful (0) [38          202         ]\nAct Deceptive (1) [409         2092        ]\n\n--- Evaluation: Ensemble (Soft) (Test Set) ---\n  Skipped (predictions not available).\n\n--- Evaluation: LSTM (Test Set) (Threshold Used: 0.230) ---\nAccuracy: 0.8646\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.16      0.13      0.15       240\nDeceptive (1)       0.92      0.93      0.93      2501\n\n     accuracy                           0.86      2741\n    macro avg       0.54      0.53      0.54      2741\n weighted avg       0.85      0.86      0.86      2741\n\n\nKey F1-Scores:\n F1[Truthful (0)]: 0.1471\n F1[Deceptive (1)]: 0.9265\n F1[Macro]: 0.5368\n F1[Weighted]: 0.8583\n\nConfusion Matrix:\n      Pred:Truthful (0)Deceptive (1)\nAct Truthful (0) [32          208         ]\nAct Deceptive (1) [163         2338        ]\n\n--- Evaluation: ENSEMBLE (Test Set) (Threshold Used: 0.440) ---\nAccuracy: 0.8971\n\nClassification Report:\n               precision    recall  f1-score   support\n\n Truthful (0)       0.18      0.05      0.08       240\nDeceptive (1)       0.91      0.98      0.95      2501\n\n     accuracy                           0.90      2741\n    macro avg       0.55      0.51      0.51      2741\n weighted avg       0.85      0.90      0.87      2741\n\n\nKey F1-Scores:\n F1[Truthful (0)]: 0.0784\n F1[Deceptive (1)]: 0.9455\n F1[Macro]: 0.5120\n F1[Weighted]: 0.8696\n\nConfusion Matrix:\n      Pred:Truthful (0)Deceptive (1)\nAct Truthful (0) [12          228         ]\nAct Deceptive (1) [54          2447        ]\n\n\n============================================================\n== Example Predictions by ENSEMBLE (Selected by True Label) ==\n============================================================\n\n--- 5 Examples with TRUE Label = TRUTHFUL ---\n\nExample (Index 7):\n  True Label:      Truthful (0)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'I'd personally rather you didn't, because a play around Munich is also a play around Trieste and Vienna. I've heard that there's some rancor over in the West and you might be able to profit from a mobbed France if you went that way....'\n\nExample (Index 50):\n  True Label:      Truthful (0)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Sure, understood.  How about this to fix it: Tys S Tun-Wes, Ion-Tun?  You take back a dot, but we push the position forwards toward Iberia?...'\n\nExample (Index 59):\n  True Label:      Truthful (0)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Yeah.  For this season, there’s no point in trying to negotiate something: the risk is too high for you on a fall turn.  But maybe once you’re turtled up and we’re looking at a slightly safer spring move, we can discuss some way to slingshot me past you...'\n\nExample (Index 116):\n  True Label:      Truthful (0)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Hello! I'm looking forward to a fun game as well. I usually see good things happen when Russia and Germany work together, so I hope we can both help each other in our initial plans. I'm assuming you're gonna try and attack Scandinavia first? Let me know your thoughts, and I look forward to us workin...'\n\nExample (Index 146):\n  True Label:      Truthful (0)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Based on what I've heard so far (I have a suspicion England might try for either Holland or Belgium). I'm gonna go for a pretty conservative start (Kiel --> Denmark, Berlin --> Kiel, Munich Hold). Do you want to agree that I take Holland and you take Belgium, then see where we go from there?...'\n\n--- 5 Examples with TRUE Label = DECEPTIVE ---\n\nExample (Index 0):\n  True Label:      Deceptive (1)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Hi Italy! Just opening up communication, and I want to know what some of your initial thoughts on the game are and if/how we can work together...'\n\nExample (Index 1):\n  True Label:      Deceptive (1)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Well....that's a great question, and a lot of it comes down to how free I'll be left to play in the West, no?...'\n\nExample (Index 2):\n  True Label:      Deceptive (1)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Well, if you want to attack France in the Mediterranean while I attack through Burgundy you can have Marseille and Iberia while I take Brest and Paris, then with France out of the way you could focus on Turkey or Austria. Sound fair?...'\n\nExample (Index 3):\n  True Label:      Deceptive (1)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Hello, I'm just asking about your move to Tyrolia. It's making me more than a little concerned...'\n\nExample (Index 4):\n  True Label:      Deceptive (1)\n  Ensemble Pred:   Deceptive (1)\n  Original Text:   'Totally understandable - but did you notice the attempt at Trieste?  Tyrolia is the natural support position for that attempt 🙂...'\n\n--- Cleaning up inference memory ---\n--- V3.1 Inference Script Finished ---\n","output_type":"stream"}],"execution_count":8}]}