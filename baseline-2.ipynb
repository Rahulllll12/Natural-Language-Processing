{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11417996,"sourceType":"datasetVersion","datasetId":7150986}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nimport re\nimport nltk\nimport torch\nimport gc\nimport numpy as np\nimport random\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report\n\nfrom transformers import (\n    RobertaTokenizer, RobertaForSequenceClassification, \n    DistilBertTokenizer, DistilBertForSequenceClassification,\n    Trainer, TrainingArguments\n)\nfrom torch.utils.data import Dataset\n\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# -------------------------------------------------\n# 1. Data Loading and Preprocessing\n# -------------------------------------------------\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\n    return ' '.join(tokens)\n\ndef load_jsonl(file_path):\n    data = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            data.append(json.loads(line))\n    df = pd.DataFrame({\n        \"messages\": [item.get(\"messages\", []) for item in data],\n        \"sender_labels\": [item.get(\"sender_labels\", []) for item in data]\n    })\n    # Filter out rows with empty labels\n    df = df[df[\"sender_labels\"].apply(lambda x: len(x) > 0)]\n    # Combine messages\n    df[\"messages\"] = df[\"messages\"].apply(lambda msgs: \" \".join(msgs))\n    # 0=Deceptive, 1=Truthful\n    df[\"labels\"] = df[\"sender_labels\"].apply(lambda lbls: int(lbls[0]))\n    return df\n\n# Adjust your paths as needed\ntrain_df = load_jsonl(\"/kaggle/input/datasetnovel1/train.jsonl\")\nval_df   = load_jsonl(\"/kaggle/input/datasetnovel1/validation.jsonl\")\ntest_df  = load_jsonl(\"/kaggle/input/datasetnovel1/test.jsonl\")\n\n# Preprocess\ntrain_df[\"clean_text\"] = train_df[\"messages\"].apply(preprocess_text)\nval_df[\"clean_text\"]   = val_df[\"messages\"].apply(preprocess_text)\ntest_df[\"clean_text\"]  = test_df[\"messages\"].apply(preprocess_text)\n\n# Upsample minority (Deceptive=0) in the training set\ndf_majority = train_df[train_df[\"labels\"] == 1]\ndf_minority = train_df[train_df[\"labels\"] == 0]\nif len(df_minority) < len(df_majority):\n    df_minority_upsampled = resample(\n        df_minority, \n        replace=True,\n        n_samples=len(df_majority),\n        random_state=SEED  # use the fixed seed\n    )\n    train_df = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1, random_state=SEED)\n\n# -------------------------------------------------\n# 2. Dataset Class\n# -------------------------------------------------\nclass DeceptionDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        item = {key: val.squeeze() for key, val in enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n# -------------------------------------------------\n# 3. Model Setup\n# -------------------------------------------------\ntrain_texts = train_df[\"clean_text\"].tolist()\ntrain_labels= train_df[\"labels\"].tolist()\nval_texts   = val_df[\"clean_text\"].tolist()\nval_labels  = val_df[\"labels\"].tolist()\ntest_texts  = test_df[\"clean_text\"].tolist()\ntest_labels = test_df[\"labels\"].tolist()\n\nclass_weights = compute_class_weight(\"balanced\", classes=[0,1], y=train_labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n\nclass WeightedRoberta(RobertaForSequenceClassification):\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n        logits = outputs.logits\n        if labels is not None:\n            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n            loss = loss_fn(logits, labels)\n            return {\"loss\": loss, \"logits\": logits}\n        return outputs\n\nclass WeightedDistilBert(DistilBertForSequenceClassification):\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n        logits = outputs.logits\n        if labels is not None:\n            loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n            loss = loss_fn(logits, labels)\n            return {\"loss\": loss, \"logits\": logits}\n        return outputs\n\nroberta_tokenizer    = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nmodel_roberta = WeightedRoberta.from_pretrained(\"roberta-base\", num_labels=2)\nmodel_distil  = WeightedDistilBert.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\ntrain_dataset_roberta = DeceptionDataset(train_texts, train_labels, roberta_tokenizer)\nval_dataset_roberta   = DeceptionDataset(val_texts,   val_labels,   roberta_tokenizer)\n\ntrain_dataset_distil  = DeceptionDataset(train_texts, train_labels, distilbert_tokenizer)\nval_dataset_distil    = DeceptionDataset(val_texts,   val_labels,   distilbert_tokenizer)\n\n# -------------------------------------------------\n# 4. Trainer & Fine-Tuning\n# -------------------------------------------------\ntorch.cuda.empty_cache()\ngc.collect()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nmodel_roberta.to(device)\nmodel_distil.to(device)\n\ntraining_args_roberta = TrainingArguments(\n    output_dir=\"./results_roberta\",\n    num_train_epochs=5,               \n    learning_rate=1e-5,               \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    logging_steps=20,\n    fp16=True,\n    report_to=\"none\"\n)\n\ntraining_args_distil = TrainingArguments(\n    output_dir=\"./results_distil\",\n    num_train_epochs=5,               \n    learning_rate=1e-5,              \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    logging_steps=20,\n    fp16=True,\n    report_to=\"none\"\n)\n\ntrainer_roberta = Trainer(\n    model=model_roberta,\n    args=training_args_roberta,\n    train_dataset=train_dataset_roberta,\n    eval_dataset=val_dataset_roberta\n)\n\ntrainer_distil = Trainer(\n    model=model_distil,\n    args=training_args_distil,\n    train_dataset=train_dataset_distil,\n    eval_dataset=val_dataset_distil\n)\n\nprint(\" Training Weighted RoBERTa...\")\ntrainer_roberta.train()\neval_rob = trainer_roberta.evaluate()\nprint(\"Roberta eval:\", eval_rob)\n\nprint(\"\\n Training Weighted DistilBERT...\")\ntrainer_distil.train()\neval_dis = trainer_distil.evaluate()\nprint(\"DistilBERT eval:\", eval_dis)\n\n# -------------------------------------------------\n# 5. Custom Thresholding on Validation\n# -------------------------------------------------\ndef get_probs(trainer, tokenizer, texts):\n    trainer.model.eval()\n    all_probs = []\n    for txt in texts:\n        enc = tokenizer(txt, truncation=True, padding='max_length',\n                        max_length=512, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = trainer.model(**enc)\n            softmax_probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n            all_probs.append(softmax_probs.cpu().numpy()[0])\n    return np.array(all_probs)\n\nval_probs_rob = get_probs(trainer_roberta,  roberta_tokenizer,    val_texts)\nval_probs_dis = get_probs(trainer_distil,   distilbert_tokenizer, val_texts)\nval_avg_probs = 0.5 * val_probs_rob + 0.5 * val_probs_dis\n\nfrom sklearn.metrics import f1_score\n\nthresholds = np.linspace(0.05, 0.95, 19)\nbest_thresh = 0.5\nbest_f1 = 0\nval_labels_array = np.array(val_labels)\nfor t in thresholds:\n    val_preds_t = (val_avg_probs[:,0] >= t).astype(int) \n    f1_0 = f1_score(val_labels_array, val_preds_t, pos_label=0)\n    if f1_0 > best_f1:\n        best_f1 = f1_0\n        best_thresh = t\n\nprint(f\"\\n Found best threshold for Deceptive=0: {best_thresh:.2f} (val F1 on class 0: {best_f1:.3f})\")\n\n# -------------------------------------------------\n# 6. Inference on Test Set with Threshold\n# -------------------------------------------------\ntest_probs_rob = get_probs(trainer_roberta, roberta_tokenizer, test_df[\"clean_text\"].tolist())\ntest_probs_dis = get_probs(trainer_distil,  distilbert_tokenizer, test_df[\"clean_text\"].tolist())\n\ntest_avg_probs = 0.5 * test_probs_rob + 0.5 * test_probs_dis\ntest_preds = (test_avg_probs[:,0] >= best_thresh).astype(int)\n\n# -------------------------------------------------\n# 7. Final Evaluation using the Paper's Style\n# -------------------------------------------------\nprint(\"\\nüîç Ensemble Model Performance on Test Set (Custom Threshold):\")\n\nreport_dict = classification_report(\n    test_df[\"labels\"],\n    test_preds,\n    target_names=[\"Deceptive\", \"Truthful\"], \n    output_dict=True\n)\n\naccuracy = report_dict[\"accuracy\"]\nmacro_f1 = report_dict[\"macro avg\"][\"f1-score\"]\nlie_f1    = report_dict[\"Deceptive\"][\"f1-score\"]\ntruth_f1  = report_dict[\"Truthful\"][\"f1-score\"]\n\nprint(f\"Accuracy:        {accuracy:.3f}\")\nprint(f\"Macro-F1:        {macro_f1:.3f}  (avg of both classes)\")\nprint(f\"Deceptive F1:    {lie_f1:.3f}    (minority class F1)\")\nprint(f\"Truthful F1:     {truth_f1:.3f}\")\n\n# print(\"\\nFull classification report:\")\n# print(classification_report(\n#     test_df[\"labels\"], \n#     test_preds, \n#     target_names=[\"Deceptive\", \"Truthful\"]\n# ))\n\nprint(\"Done!\")\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-15T15:30:00.128691Z","iopub.execute_input":"2025-04-15T15:30:00.128999Z","iopub.status.idle":"2025-04-15T15:34:01.634905Z","shell.execute_reply.started":"2025-04-15T15:30:00.128975Z","shell.execute_reply":"2025-04-15T15:34:01.634145Z"},"trusted":true},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nSome weights of WeightedRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of WeightedDistilBert were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n Training Weighted RoBERTa...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [220/220 02:08, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.688800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.578400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.495100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.374600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.134000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.102400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.102600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.007100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.007100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.075500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Roberta eval: {'eval_loss': 0.7002055048942566, 'eval_runtime': 0.5895, 'eval_samples_per_second': 33.926, 'eval_steps_per_second': 5.089, 'epoch': 5.0}\n\n Training Weighted DistilBERT...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [220/220 01:16, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.695900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.650800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.557600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.426900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.303300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.172600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.127100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.054300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.042000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.036000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"DistilBERT eval: {'eval_loss': 0.27284297347068787, 'eval_runtime': 0.4879, 'eval_samples_per_second': 40.996, 'eval_steps_per_second': 6.149, 'epoch': 5.0}\n\n Found best threshold for Deceptive=0: 0.05 (val F1 on class 0: 0.118)\n\nüîç Ensemble Model Performance on Test Set (Custom Threshold):\nAccuracy:        0.333\nMacro-F1:        0.308  (avg of both classes)\nDeceptive F1:    0.176    (minority class F1)\nTruthful F1:     0.440\nDone!\n","output_type":"stream"}],"execution_count":5}]}